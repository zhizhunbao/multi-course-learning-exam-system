## ğŸ“š æ¦‚å¿µ

### å¼‚å¸¸å€¼æ ¸å¿ƒæ¦‚å¿µ

- **å¼‚å¸¸å€¼ï¼ˆOutlierï¼‰**ï¼šæ˜¾è‘—åç¦»æ•°æ®é›†ä¸­å…¶ä»–è§‚æµ‹å€¼çš„æ•°æ®ç‚¹
- **ç¦»ç¾¤ç‚¹æ£€æµ‹ï¼ˆOutlier Detectionï¼‰**ï¼šè¯†åˆ«ä¸ç¬¦åˆé¢„æœŸæ¨¡å¼çš„æ•°æ®ç‚¹
- **å¼‚å¸¸æ£€æµ‹ï¼ˆAnomaly Detectionï¼‰**ï¼šå‘ç°ç½•è§äº‹ä»¶ã€è§‚æµ‹æˆ–å¯ç–‘é¡¹
- **æ–°å¥‡æ£€æµ‹ï¼ˆNovelty Detectionï¼‰**ï¼šè¯†åˆ«ä¸è®­ç»ƒæ•°æ®ä¸åŒçš„æ–°æ¨¡å¼

### å¼‚å¸¸å€¼ç±»å‹

1. **ç‚¹å¼‚å¸¸å€¼ï¼ˆPoint Anomaliesï¼‰**

   - å•ä¸ªæ•°æ®ç‚¹å¼‚å¸¸
   - æœ€å¸¸è§çš„å¼‚å¸¸ç±»å‹
   - ç¤ºä¾‹ï¼šä¿¡ç”¨å¡äº¤æ˜“é‡‘é¢å¼‚å¸¸é«˜

2. **ä¸Šä¸‹æ–‡å¼‚å¸¸å€¼ï¼ˆContextual Anomaliesï¼‰**

   - åœ¨ç‰¹å®šä¸Šä¸‹æ–‡ä¸­å¼‚å¸¸
   - ä¾èµ–äºç¯å¢ƒ
   - ç¤ºä¾‹ï¼šå¤å­£æ¸©åº¦-10Â°C

3. **é›†ä½“å¼‚å¸¸å€¼ï¼ˆCollective Anomaliesï¼‰**
   - ä¸€ç»„æ•°æ®ç‚¹å…±åŒå¼‚å¸¸
   - å•ä¸ªç‚¹å¯èƒ½æ­£å¸¸
   - ç¤ºä¾‹ï¼šç½‘ç»œæµé‡çš„å¼‚å¸¸æ¨¡å¼

### å¼‚å¸¸å€¼äº§ç”ŸåŸå› 

- **æ•°æ®è¾“å…¥é”™è¯¯**ï¼šæ‰‹åŠ¨å½•å…¥é”™è¯¯
- **æµ‹é‡é”™è¯¯**ï¼šä¼ æ„Ÿå™¨æ•…éšœã€ä»ªå™¨è¯¯å·®
- **å®éªŒé”™è¯¯**ï¼šæ ·æœ¬å¤„ç†ä¸å½“
- **æ•°æ®å¤„ç†é”™è¯¯**ï¼šè®¡ç®—æˆ–è½¬æ¢é”™è¯¯
- **è‡ªç„¶å˜å¼‚**ï¼šçœŸå®çš„æç«¯å€¼
- **æ¬ºè¯ˆè¡Œä¸º**ï¼šæ¶æ„æ´»åŠ¨

## ğŸ” è§£é‡Š

### ç»Ÿè®¡æ–¹æ³•

**1. Z åˆ†æ•°æ³•ï¼ˆZ-Score Methodï¼‰**
$$z = \frac{x - \mu}{\sigma}$$

- **è§„åˆ™**ï¼š|z| > 3 é€šå¸¸è¢«è®¤ä¸ºæ˜¯å¼‚å¸¸å€¼
- **å‡è®¾**ï¼šæ•°æ®æœä»æ­£æ€åˆ†å¸ƒ
- **ä¼˜ç‚¹**ï¼šç®€å•ã€ç›´è§‚
- **ç¼ºç‚¹**ï¼šå¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼ˆå‡å€¼å’Œæ ‡å‡†å·®å—å½±å“ï¼‰

```python
from scipy import stats
import numpy as np

def detect_outliers_zscore(data, threshold=3):
    z_scores = np.abs(stats.zscore(data))
    return np.where(z_scores > threshold)[0]

# ç¤ºä¾‹
data = [10, 12, 14, 12, 11, 15, 100, 13, 12]
outliers = detect_outliers_zscore(data)
print(f'å¼‚å¸¸å€¼ç´¢å¼•: {outliers}')
```

**2. ä¿®æ­£ Z åˆ†æ•°ï¼ˆModified Z-Scoreï¼‰**
ä½¿ç”¨ä¸­ä½æ•°ç»å¯¹åå·®ï¼ˆMADï¼‰ï¼š
$$Modified\ Z = \frac{0.6745(x_i - median)}{MAD}$$
$$MAD = median(|x_i - median|)$$

- **ä¼˜ç‚¹**ï¼šå¯¹å¼‚å¸¸å€¼æ›´ç¨³å¥
- **é˜ˆå€¼**ï¼šé€šå¸¸ä½¿ç”¨ 3.5

**3. IQR æ–¹æ³•ï¼ˆInterquartile Rangeï¼‰**
$$IQR = Q3 - Q1$$
$$ä¸‹ç•Œ = Q1 - 1.5 \times IQR$$
$$ä¸Šç•Œ = Q3 + 1.5 \times IQR$$

- **Q1**ï¼šç¬¬ä¸€å››åˆ†ä½æ•°ï¼ˆ25%ï¼‰
- **Q3**ï¼šç¬¬ä¸‰å››åˆ†ä½æ•°ï¼ˆ75%ï¼‰
- **ä¼˜ç‚¹**ï¼šå¯¹å¼‚å¸¸å€¼ç¨³å¥ï¼Œæ— åˆ†å¸ƒå‡è®¾
- **å¯è§†åŒ–**ï¼šç®±çº¿å›¾

```python
def detect_outliers_iqr(data):
    Q1 = np.percentile(data, 25)
    Q3 = np.percentile(data, 75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    outliers = []
    for i, x in enumerate(data):
        if x < lower_bound or x > upper_bound:
            outliers.append(i)

    return outliers, lower_bound, upper_bound

# ç¤ºä¾‹
data = [10, 12, 14, 12, 11, 15, 100, 13, 12]
outliers, lower, upper = detect_outliers_iqr(data)
print(f'å¼‚å¸¸å€¼ç´¢å¼•: {outliers}')
print(f'æ­£å¸¸èŒƒå›´: [{lower:.2f}, {upper:.2f}]')
```

### åŸºäºè·ç¦»çš„æ–¹æ³•

**1. k è¿‘é‚»è·ç¦»ï¼ˆk-NN Distanceï¼‰**

- è®¡ç®—æ¯ä¸ªç‚¹åˆ°ç¬¬ k ä¸ªæœ€è¿‘é‚»çš„è·ç¦»
- è·ç¦»å¤§çš„ç‚¹è¢«è®¤ä¸ºæ˜¯å¼‚å¸¸å€¼
- **ä¼˜ç‚¹**ï¼šç®€å•ã€æ— å‚æ•°å‡è®¾
- **ç¼ºç‚¹**ï¼šè®¡ç®—æˆæœ¬é«˜ã€å¯¹ k å€¼æ•æ„Ÿ

**2. å±€éƒ¨å¼‚å¸¸å› å­ï¼ˆLOF - Local Outlier Factorï¼‰**
$$LOF(A) = \frac{\sum_{B \in N_k(A)} \frac{lrd(B)}{lrd(A)}}{|N_k(A)|}$$

å…¶ä¸­ï¼š

- lrd = å±€éƒ¨å¯è¾¾å¯†åº¦
- N_k(A) = A çš„ k ä¸ªæœ€è¿‘é‚»

**è§£é‡Šï¼š**

- LOF â‰ˆ 1ï¼šä¸é‚»å±…å¯†åº¦ç›¸ä¼¼ï¼ˆæ­£å¸¸ï¼‰
- LOF >> 1ï¼šå¯†åº¦æ¯”é‚»å±…ä½å¾—å¤šï¼ˆå¼‚å¸¸ï¼‰
- LOF < 1ï¼šå¯†åº¦æ¯”é‚»å±…é«˜ï¼ˆæ­£å¸¸ï¼‰

```python
from sklearn.neighbors import LocalOutlierFactor

# åˆ›å»ºLOFæ£€æµ‹å™¨
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)

# é¢„æµ‹ï¼ˆ-1è¡¨ç¤ºå¼‚å¸¸ï¼Œ1è¡¨ç¤ºæ­£å¸¸ï¼‰
predictions = lof.fit_predict(X)

# è·å–å¼‚å¸¸åˆ†æ•°
scores = lof.negative_outlier_factor_

# æ‰¾åˆ°å¼‚å¸¸å€¼
outliers = np.where(predictions == -1)[0]
print(f'æ£€æµ‹åˆ° {len(outliers)} ä¸ªå¼‚å¸¸å€¼')
```

### åŸºäºå¯†åº¦çš„æ–¹æ³•

**DBSCANï¼ˆDensity-Based Spatial Clusteringï¼‰**

- ä¸å±äºä»»ä½•ç°‡çš„ç‚¹è¢«æ ‡è®°ä¸ºå™ªå£°/å¼‚å¸¸å€¼
- **å‚æ•°**ï¼š
  - epsï¼šé‚»åŸŸåŠå¾„
  - min_samplesï¼šæœ€å°ç‚¹æ•°

```python
from sklearn.cluster import DBSCAN

# åˆ›å»ºDBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)

# æ‹Ÿåˆ
labels = dbscan.fit_predict(X)

# å™ªå£°ç‚¹æ ‡è®°ä¸º-1
outliers = np.where(labels == -1)[0]
print(f'æ£€æµ‹åˆ° {len(outliers)} ä¸ªå¼‚å¸¸å€¼')
```

### æœºå™¨å­¦ä¹ æ–¹æ³•

**1. éš”ç¦»æ£®æ—ï¼ˆIsolation Forestï¼‰**
**åŸç†ï¼š**

- å¼‚å¸¸å€¼æ›´å®¹æ˜“è¢«"éš”ç¦»"
- æ„å»ºéšæœºå†³ç­–æ ‘
- å¼‚å¸¸å€¼çš„å¹³å‡è·¯å¾„é•¿åº¦è¾ƒçŸ­

```python
from sklearn.ensemble import IsolationForest

# åˆ›å»ºéš”ç¦»æ£®æ—
iso_forest = IsolationForest(
    n_estimators=100,
    contamination=0.1,  # å¼‚å¸¸å€¼æ¯”ä¾‹
    random_state=42
)

# è®­ç»ƒå’Œé¢„æµ‹
predictions = iso_forest.fit_predict(X)

# -1è¡¨ç¤ºå¼‚å¸¸ï¼Œ1è¡¨ç¤ºæ­£å¸¸
outliers = np.where(predictions == -1)[0]
print(f'æ£€æµ‹åˆ° {len(outliers)} ä¸ªå¼‚å¸¸å€¼')

# è·å–å¼‚å¸¸åˆ†æ•°
scores = iso_forest.score_samples(X)
```

**2. å•ç±»æ”¯æŒå‘é‡æœºï¼ˆOne-Class SVMï¼‰**
**åŸç†ï¼š**

- æ‰¾åˆ°åŒ…å«å¤§å¤šæ•°æ•°æ®çš„è¶…çƒæˆ–è¶…å¹³é¢
- ä¸åœ¨è¾¹ç•Œå†…çš„ç‚¹æ˜¯å¼‚å¸¸å€¼

```python
from sklearn.svm import OneClassSVM

# åˆ›å»ºOne-Class SVM
ocsvm = OneClassSVM(
    kernel='rbf',
    gamma='auto',
    nu=0.1  # å¼‚å¸¸å€¼ä¸Šç•Œ
)

# è®­ç»ƒå’Œé¢„æµ‹
predictions = ocsvm.fit_predict(X)

# -1è¡¨ç¤ºå¼‚å¸¸ï¼Œ1è¡¨ç¤ºæ­£å¸¸
outliers = np.where(predictions == -1)[0]
```

**3. è‡ªåŠ¨ç¼–ç å™¨ï¼ˆAutoencoderï¼‰**
**åŸç†ï¼š**

- ç¥ç»ç½‘ç»œå­¦ä¹ æ•°æ®å‹ç¼©å’Œé‡å»º
- é‡å»ºè¯¯å·®å¤§çš„ç‚¹æ˜¯å¼‚å¸¸å€¼

```python
from tensorflow import keras
from tensorflow.keras import layers

# æ„å»ºè‡ªåŠ¨ç¼–ç å™¨
encoder = keras.Sequential([
    layers.Dense(32, activation='relu', input_shape=(n_features,)),
    layers.Dense(16, activation='relu'),
    layers.Dense(8, activation='relu')
])

decoder = keras.Sequential([
    layers.Dense(16, activation='relu', input_shape=(8,)),
    layers.Dense(32, activation='relu'),
    layers.Dense(n_features, activation='sigmoid')
])

autoencoder = keras.Sequential([encoder, decoder])
autoencoder.compile(optimizer='adam', loss='mse')

# è®­ç»ƒ
autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_split=0.1)

# é¢„æµ‹å’Œè®¡ç®—é‡å»ºè¯¯å·®
reconstructions = autoencoder.predict(X_test)
mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)

# è®¾ç½®é˜ˆå€¼
threshold = np.percentile(mse, 95)
outliers = np.where(mse > threshold)[0]
```

### å¤„ç†å¼‚å¸¸å€¼ç­–ç•¥

**1. åˆ é™¤**

- é€‚ç”¨ï¼šæ•°æ®å……è¶³ã€å¼‚å¸¸å€¼ç¡®å®æ˜¯é”™è¯¯
- é£é™©ï¼šä¸¢å¤±ä¿¡æ¯

**2. æ›¿æ¢/æ’è¡¥**

- ç”¨å‡å€¼ã€ä¸­ä½æ•°æˆ–æ’å€¼æ›¿æ¢
- é€‚ç”¨ï¼šå¼‚å¸¸å€¼ä¸å¤šã€éœ€è¦ä¿ç•™æ•°æ®é‡

**3. è½¬æ¢**

- å¯¹æ•°è½¬æ¢ã€å¹³æ–¹æ ¹è½¬æ¢
- å‡å°‘å¼‚å¸¸å€¼å½±å“

**4. åˆ†ç®±**

- å°†è¿ç»­å€¼ç¦»æ•£åŒ–
- å¼‚å¸¸å€¼å½’å…¥æå€¼ç®±

**5. ä¿ç•™**

- å¼‚å¸¸å€¼å¯èƒ½æ˜¯é‡è¦ä¿¡æ¯
- ä½¿ç”¨ç¨³å¥ç®—æ³•

**6. å•ç‹¬å»ºæ¨¡**

- ä¸ºå¼‚å¸¸å€¼å»ºç«‹å•ç‹¬æ¨¡å‹
- é€‚ç”¨ï¼šå¼‚å¸¸å€¼æœ‰ç‰¹æ®Šæ„ä¹‰

## ğŸ“œ å†å²

### å¼‚å¸¸å€¼æ£€æµ‹çš„å‘å±•

**19 ä¸–çºª - ç»Ÿè®¡èµ·æº**

- 1852 å¹´ï¼š**Benjamin Peirce** æå‡ºç¬¬ä¸€ä¸ªå¼‚å¸¸å€¼æ£€æµ‹å‡†åˆ™
- ç»Ÿè®¡å­¦ä¸­çš„å¼‚å¸¸å€¼æ¦‚å¿µå¼€å§‹å½¢æˆ

**1950-1960 å¹´ä»£ - ç†è®ºå‘å±•**

- 1950 å¹´ï¼š**Grubbs æ£€éªŒ** è¢«æå‡º
- ç»Ÿè®¡æ£€éªŒæ–¹æ³•æˆç†Ÿ
- ç®±çº¿å›¾æ¦‚å¿µè¯ç”Ÿ

**1970-1980 å¹´ä»£ - ç¨³å¥ç»Ÿè®¡**

- ç¨³å¥ç»Ÿè®¡æ–¹æ³•å‘å±•
- **John Tukey (1977)**ï¼šæå‡ºç®±çº¿å›¾å’Œ IQR æ–¹æ³•
- å¯¹å¼‚å¸¸å€¼ç¨³å¥çš„ä¼°è®¡æ–¹æ³•

**1990 å¹´ä»£ - è·ç¦»å’Œå¯†åº¦æ–¹æ³•**

- **Knorr & Ng (1998)**ï¼šåŸºäºè·ç¦»çš„å¼‚å¸¸å€¼æ£€æµ‹
- **Breunig et al. (2000)**ï¼šæå‡º LOF ç®—æ³•
- DBSCAN ç”¨äºå¼‚å¸¸æ£€æµ‹

**2000 å¹´ä»£ - æœºå™¨å­¦ä¹ æ–¹æ³•**

- **Isolation Forest (2008)**ï¼šåˆ˜é£é¾™ç­‰äººæå‡º
- One-Class SVM åº”ç”¨
- é›†æˆæ–¹æ³•å‘å±•

**2010 å¹´ä»£è‡³ä»Š - æ·±åº¦å­¦ä¹ æ—¶ä»£**

- æ·±åº¦è‡ªåŠ¨ç¼–ç å™¨
- ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç”¨äºå¼‚å¸¸æ£€æµ‹
- æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹
- å®æ—¶å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ

### å…³é”®è´¡çŒ®è€…

- **John Tukey**ï¼šç®±çº¿å›¾ã€ç¨³å¥ç»Ÿè®¡
- **Breunig ç­‰äºº**ï¼šLOF ç®—æ³•
- **åˆ˜é£é¾™ï¼ˆFei Tony Liuï¼‰**ï¼šéš”ç¦»æ£®æ—

## ğŸ’ª ç»ƒä¹ 

### åŸºç¡€ç»ƒä¹ 

**ç»ƒä¹  1ï¼šZ åˆ†æ•°è®¡ç®—**
ç»™å®šæ•°æ®ï¼š[10, 12, 14, 16, 18, 20, 100]

1. è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
2. è®¡ç®—æ¯ä¸ªç‚¹çš„ Z åˆ†æ•°
3. è¯†åˆ«å¼‚å¸¸å€¼ï¼ˆ|z| > 3ï¼‰

**ç»ƒä¹  2ï¼šIQR æ–¹æ³•**
ç»™å®šæ•°æ®ï¼š[5, 7, 8, 9, 10, 11, 12, 13, 15, 100]

1. è®¡ç®— Q1ã€Q3 å’Œ IQR
2. ç¡®å®šå¼‚å¸¸å€¼è¾¹ç•Œ
3. è¯†åˆ«å¼‚å¸¸å€¼

**ç»ƒä¹  3ï¼šå¯è§†åŒ–å¼‚å¸¸å€¼**
åˆ›å»ºç®±çº¿å›¾å’Œæ•£ç‚¹å›¾è¯†åˆ«ä»¥ä¸‹æ•°æ®çš„å¼‚å¸¸å€¼ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(42)
data = np.concatenate([np.random.normal(50, 10, 100), [120, 125, -10]])
```

### å®è·µé¡¹ç›®

**é¡¹ç›® 1ï¼šå¤šæ–¹æ³•å¼‚å¸¸å€¼æ£€æµ‹æ¯”è¾ƒ**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from scipy import stats

# ç”Ÿæˆæ•°æ®
np.random.seed(42)
X_normal = np.random.normal(0, 1, (300, 2))
X_outliers = np.random.uniform(-4, 4, (20, 2))
X = np.vstack([X_normal, X_outliers])

# æ–¹æ³•1ï¼šZ-Score
z_scores = np.abs(stats.zscore(X))
z_outliers = np.where((z_scores > 3).any(axis=1))[0]

# æ–¹æ³•2ï¼šIQR
def iqr_outliers(data):
    Q1 = np.percentile(data, 25, axis=0)
    Q3 = np.percentile(data, 75, axis=0)
    IQR = Q3 - Q1
    mask = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)
    return np.where(mask)[0]

iqr_out = iqr_outliers(X)

# æ–¹æ³•3ï¼šIsolation Forest
iso = IsolationForest(contamination=0.1, random_state=42)
iso_pred = iso.fit_predict(X)
iso_out = np.where(iso_pred == -1)[0]

# æ–¹æ³•4ï¼šLOF
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
lof_pred = lof.fit_predict(X)
lof_out = np.where(lof_pred == -1)[0]

# æ–¹æ³•5ï¼šOne-Class SVM
ocsvm = OneClassSVM(nu=0.1)
ocsvm_pred = ocsvm.fit_predict(X)
ocsvm_out = np.where(ocsvm_pred == -1)[0]

# å¯è§†åŒ–æ¯”è¾ƒ
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
methods = [
    ('åŸå§‹æ•°æ®', None),
    ('Z-Score', z_outliers),
    ('IQR', iqr_out),
    ('Isolation Forest', iso_out),
    ('LOF', lof_out),
    ('One-Class SVM', ocsvm_out)
]

for ax, (title, outliers) in zip(axes.flat, methods):
    ax.scatter(X[:, 0], X[:, 1], c='blue', s=20, alpha=0.5)
    if outliers is not None:
        ax.scatter(X[outliers, 0], X[outliers, 1], c='red', s=100, marker='x')
    ax.set_title(f'{title}\næ£€æµ‹åˆ° {len(outliers) if outliers is not None else 0} ä¸ªå¼‚å¸¸å€¼')
    ax.set_xlabel('ç‰¹å¾1')
    ax.set_ylabel('ç‰¹å¾2')

plt.tight_layout()
plt.savefig('outlier_comparison.png')
plt.show()
```

**é¡¹ç›® 2ï¼šä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹**

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
from sklearn.metrics import classification_report, confusion_matrix

# åŠ è½½æ•°æ®ï¼ˆå‡è®¾æœ‰æ ‡è®°çš„æ¬ºè¯ˆæ•°æ®ï¼‰
# df = pd.read_csv('creditcard.csv')

# ç‰¹å¾å’Œæ ‡ç­¾
X = df.drop('Class', axis=1)
y = df['Class']

# åˆ†å‰²æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# è®­ç»ƒéš”ç¦»æ£®æ—ï¼ˆåªç”¨æ­£å¸¸æ•°æ®ï¼‰
iso_forest = IsolationForest(
    n_estimators=100,
    contamination=0.01,  # å‡è®¾1%æ˜¯æ¬ºè¯ˆ
    random_state=42
)
iso_forest.fit(X_train_scaled[y_train == 0])  # åªç”¨æ­£å¸¸äº¤æ˜“è®­ç»ƒ

# é¢„æµ‹
y_pred = iso_forest.predict(X_test_scaled)
y_pred = np.where(y_pred == -1, 1, 0)  # è½¬æ¢ä¸º0/1

# è¯„ä¼°
print('æ··æ·†çŸ©é˜µ:')
print(confusion_matrix(y_test, y_pred))
print('\nåˆ†ç±»æŠ¥å‘Š:')
print(classification_report(y_test, y_pred))

# åˆ†æå¼‚å¸¸åˆ†æ•°
scores = iso_forest.score_samples(X_test_scaled)
plt.figure(figsize=(10, 6))
plt.hist(scores[y_test == 0], bins=50, alpha=0.5, label='æ­£å¸¸')
plt.hist(scores[y_test == 1], bins=50, alpha=0.5, label='æ¬ºè¯ˆ')
plt.xlabel('å¼‚å¸¸åˆ†æ•°')
plt.ylabel('é¢‘æ•°')
plt.legend()
plt.title('å¼‚å¸¸åˆ†æ•°åˆ†å¸ƒ')
plt.show()
```

**é¡¹ç›® 3ï¼šæ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
np.random.seed(42)
dates = pd.date_range('2023-01-01', periods=365, freq='D')
trend = np.linspace(100, 150, 365)
seasonal = 10 * np.sin(2 * np.pi * np.arange(365) / 365)
noise = np.random.normal(0, 2, 365)
data = trend + seasonal + noise

# æ·»åŠ å¼‚å¸¸å€¼
data[50] += 30  # ç‚¹å¼‚å¸¸
data[100] += 25
data[200:210] -= 20  # é›†ä½“å¼‚å¸¸

df = pd.DataFrame({'date': dates, 'value': data})
df.set_index('date', inplace=True)

# æ–¹æ³•1ï¼šç§»åŠ¨å¹³å‡å’Œæ ‡å‡†å·®
window = 30
df['rolling_mean'] = df['value'].rolling(window=window).mean()
df['rolling_std'] = df['value'].rolling(window=window).std()
df['upper_bound'] = df['rolling_mean'] + 3 * df['rolling_std']
df['lower_bound'] = df['rolling_mean'] - 3 * df['rolling_std']
df['anomaly'] = ((df['value'] > df['upper_bound']) | (df['value'] < df['lower_bound']))

# å¯è§†åŒ–
plt.figure(figsize=(15, 6))
plt.plot(df.index, df['value'], label='åŸå§‹æ•°æ®', alpha=0.7)
plt.plot(df.index, df['rolling_mean'], label='ç§»åŠ¨å¹³å‡', color='green')
plt.fill_between(df.index, df['upper_bound'], df['lower_bound'], alpha=0.2, color='gray')
plt.scatter(df.index[df['anomaly']], df['value'][df['anomaly']],
           color='red', s=100, label='å¼‚å¸¸å€¼', zorder=5)
plt.xlabel('æ—¥æœŸ')
plt.ylabel('å€¼')
plt.title('æ—¶é—´åºåˆ—å¼‚å¸¸æ£€æµ‹')
plt.legend()
plt.tight_layout()
plt.show()

print(f'æ£€æµ‹åˆ° {df["anomaly"].sum()} ä¸ªå¼‚å¸¸ç‚¹')
```

## ğŸ¯ æµ‹ä¸€æµ‹

### é€‰æ‹©é¢˜

**1. Z åˆ†æ•°æ³•å‡è®¾æ•°æ®æœä»ä»€ä¹ˆåˆ†å¸ƒï¼Ÿ**

- A. å‡åŒ€åˆ†å¸ƒ
- B. æ­£æ€åˆ†å¸ƒ
- C. æ³Šæ¾åˆ†å¸ƒ
- D. æŒ‡æ•°åˆ†å¸ƒ

**2. IQR æ–¹æ³•ä¸­ï¼Œé€šå¸¸è®¤ä¸ºè¶…è¿‡ Q3 + 1.5Ã—IQR çš„ç‚¹æ˜¯ï¼š**

- A. æ­£å¸¸å€¼
- B. å¼‚å¸¸å€¼
- C. ç¼ºå¤±å€¼
- D. ä¸­ä½æ•°

**3. LOF ç®—æ³•åŸºäºä»€ä¹ˆåŸç†ï¼Ÿ**

- A. è·ç¦»
- B. å¯†åº¦
- C. ç»Ÿè®¡æ£€éªŒ
- D. èšç±»

**4. éš”ç¦»æ£®æ—ä¸­ï¼Œå¼‚å¸¸å€¼çš„ç‰¹ç‚¹æ˜¯ï¼š**

- A. è·¯å¾„é•¿åº¦è¾ƒé•¿
- B. è·¯å¾„é•¿åº¦è¾ƒçŸ­
- C. å¯†åº¦è¾ƒé«˜
- D. è·ç¦»è¾ƒè¿‘

**5. ä»¥ä¸‹å“ªä¸ªä¸æ˜¯å¤„ç†å¼‚å¸¸å€¼çš„æ–¹æ³•ï¼Ÿ**

- A. åˆ é™¤
- B. æ›¿æ¢
- C. è½¬æ¢
- D. å¤åˆ¶

**6. ä¸Šä¸‹æ–‡å¼‚å¸¸å€¼æ˜¯æŒ‡ï¼š**

- A. å•ä¸ªç‚¹å¼‚å¸¸
- B. åœ¨ç‰¹å®šä¸Šä¸‹æ–‡ä¸­å¼‚å¸¸
- C. ä¸€ç»„ç‚¹å¼‚å¸¸
- D. æ‰€æœ‰ç‚¹éƒ½å¼‚å¸¸

**7. One-Class SVM çš„ nu å‚æ•°æ§åˆ¶ï¼š**

- A. å¼‚å¸¸å€¼ä¸‹ç•Œ
- B. å¼‚å¸¸å€¼ä¸Šç•Œ
- C. å¹³å‡å€¼
- D. æ–¹å·®

**8. è‡ªåŠ¨ç¼–ç å™¨æ£€æµ‹å¼‚å¸¸å€¼åŸºäºï¼š**

- A. è·ç¦»
- B. å¯†åº¦
- C. é‡å»ºè¯¯å·®
- D. åˆ†ç±»æ¦‚ç‡

### åˆ¤æ–­é¢˜

**1. æ‰€æœ‰å¼‚å¸¸å€¼éƒ½åº”è¯¥è¢«åˆ é™¤ã€‚** ï¼ˆÃ—ï¼‰

**2. Z åˆ†æ•°æ³•å¯¹å¼‚å¸¸å€¼æœ¬èº«å¾ˆæ•æ„Ÿã€‚** ï¼ˆâœ“ï¼‰

**3. IQR æ–¹æ³•ä¸éœ€è¦å‡è®¾æ•°æ®åˆ†å¸ƒã€‚** ï¼ˆâœ“ï¼‰

**4. LOF = 1 è¡¨ç¤ºè¯¥ç‚¹æ˜¯å¼‚å¸¸å€¼ã€‚** ï¼ˆÃ—ï¼‰

**5. éš”ç¦»æ£®æ—é€‚åˆé«˜ç»´æ•°æ®ã€‚** ï¼ˆâœ“ï¼‰

**6. å¼‚å¸¸å€¼æ£€æµ‹åªèƒ½ç”¨äºæ•°å€¼æ•°æ®ã€‚** ï¼ˆÃ—ï¼‰

**7. ä¿®æ­£ Z åˆ†æ•°æ¯”æ™®é€š Z åˆ†æ•°æ›´ç¨³å¥ã€‚** ï¼ˆâœ“ï¼‰

**8. DBSCAN å¯ä»¥ç”¨äºå¼‚å¸¸å€¼æ£€æµ‹ã€‚** ï¼ˆâœ“ï¼‰

### ç®€ç­”é¢˜

**1. æ¯”è¾ƒ Z åˆ†æ•°æ³•å’Œ IQR æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ã€‚**

**å‚è€ƒç­”æ¡ˆï¼š**
Z åˆ†æ•°æ³•ï¼š

- ä¼˜ç‚¹ï¼šç®€å•ç›´è§‚ï¼Œè®¡ç®—å¿«é€Ÿ
- ç¼ºç‚¹ï¼šå‡è®¾æ­£æ€åˆ†å¸ƒï¼Œå¯¹å¼‚å¸¸å€¼æ•æ„Ÿï¼ˆå‡å€¼å’Œæ ‡å‡†å·®å—å½±å“ï¼‰

IQR æ–¹æ³•ï¼š

- ä¼˜ç‚¹ï¼šå¯¹å¼‚å¸¸å€¼ç¨³å¥ï¼Œæ— åˆ†å¸ƒå‡è®¾ï¼Œé€‚ç”¨èŒƒå›´å¹¿
- ç¼ºç‚¹ï¼šå¯èƒ½ä¸å¤Ÿæ•æ„Ÿï¼Œåªè€ƒè™‘å•å˜é‡

**2. è§£é‡Š LOF ç®—æ³•çš„å·¥ä½œåŸç†ã€‚**

**å‚è€ƒç­”æ¡ˆï¼š**
LOFï¼ˆå±€éƒ¨å¼‚å¸¸å› å­ï¼‰æ¯”è¾ƒæ¯ä¸ªç‚¹çš„å±€éƒ¨å¯†åº¦ä¸å…¶é‚»å±…çš„å±€éƒ¨å¯†åº¦ã€‚å¦‚æœä¸€ä¸ªç‚¹çš„å¯†åº¦æ˜¾è‘—ä½äºå…¶é‚»å±…ï¼Œå®ƒè¢«è®¤ä¸ºæ˜¯å¼‚å¸¸å€¼ã€‚LOFâ‰ˆ1 è¡¨ç¤ºæ­£å¸¸ï¼ŒLOF>>1 è¡¨ç¤ºå¼‚å¸¸ã€‚è¯¥æ–¹æ³•å¯ä»¥æ£€æµ‹å±€éƒ¨å¼‚å¸¸å€¼ï¼Œé€‚åˆå¯†åº¦ä¸å‡åŒ€çš„æ•°æ®é›†ã€‚

**3. åœ¨ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥ä¿ç•™å¼‚å¸¸å€¼ï¼Ÿ**

**å‚è€ƒç­”æ¡ˆï¼š**

- å¼‚å¸¸å€¼ä»£è¡¨çœŸå®ç°è±¡ï¼ˆå¦‚æç«¯å¤©æ°”ï¼‰
- å¼‚å¸¸å€¼æ˜¯ç ”ç©¶é‡ç‚¹ï¼ˆå¦‚æ¬ºè¯ˆæ£€æµ‹ï¼‰
- æ•°æ®é‡å°ï¼Œåˆ é™¤ä¼šæŸå¤±è¿‡å¤šä¿¡æ¯
- ä½¿ç”¨ç¨³å¥ç®—æ³•ï¼Œå¼‚å¸¸å€¼å½±å“å°
- éœ€è¦ç†è§£å¼‚å¸¸å€¼çš„æˆå› 
- å¼‚å¸¸å€¼åŒ…å«é‡è¦å•†ä¸šæ´å¯Ÿ

### è®¡ç®—é¢˜

**1. ç»™å®šæ•°æ®[10, 12, 14, 16, 18, 100]ï¼Œä½¿ç”¨ Z åˆ†æ•°æ³•è¯†åˆ«å¼‚å¸¸å€¼ã€‚**

**å‚è€ƒç­”æ¡ˆï¼š**

1. å‡å€¼ Î¼ = (10+12+14+16+18+100)/6 = 28.33
2. æ ‡å‡†å·® Ïƒ = 33.11
3. Z åˆ†æ•°ï¼š
   - 10: (10-28.33)/33.11 = -0.55
   - 12: -0.49
   - 14: -0.43
   - 16: -0.37
   - 18: -0.31
   - 100: (100-28.33)/33.11 = 2.16

ç»“è®ºï¼šä½¿ç”¨|z|>3 çš„æ ‡å‡†ï¼Œæ²¡æœ‰å¼‚å¸¸å€¼ã€‚ä½† 100 æ˜æ˜¾åç¦»ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´é˜ˆå€¼ã€‚

**2. ç»™å®šæ•°æ®[5, 7, 8, 10, 12, 14, 15, 18, 20, 50]ï¼Œä½¿ç”¨ IQR æ–¹æ³•è¯†åˆ«å¼‚å¸¸å€¼ã€‚**

**å‚è€ƒç­”æ¡ˆï¼š**

1. æ’åºï¼š[5, 7, 8, 10, 12, 14, 15, 18, 20, 50]
2. Q1 = 8.5, Q3 = 18.5
3. IQR = 18.5 - 8.5 = 10
4. ä¸‹ç•Œ = 8.5 - 1.5Ã—10 = -6.5
5. ä¸Šç•Œ = 18.5 + 1.5Ã—10 = 33.5
6. å¼‚å¸¸å€¼ï¼š50ï¼ˆè¶…è¿‡ä¸Šç•Œï¼‰

## ğŸ—ºï¸ æ€ç»´å¯¼å›¾

```
å¼‚å¸¸å€¼æ£€æµ‹
â”‚
â”œâ”€â”€â”€ å¼‚å¸¸å€¼ç±»å‹
â”‚    â”œâ”€â”€â”€ ç‚¹å¼‚å¸¸å€¼
â”‚    â”œâ”€â”€â”€ ä¸Šä¸‹æ–‡å¼‚å¸¸å€¼
â”‚    â””â”€â”€â”€ é›†ä½“å¼‚å¸¸å€¼
â”‚
â”œâ”€â”€â”€ ç»Ÿè®¡æ–¹æ³•
â”‚    â”œâ”€â”€â”€ Zåˆ†æ•°æ³•
â”‚    â”‚    â”œâ”€â”€â”€ |z| > 3
â”‚    â”‚    â””â”€â”€â”€ å‡è®¾æ­£æ€åˆ†å¸ƒ
â”‚    â”œâ”€â”€â”€ ä¿®æ­£Zåˆ†æ•°
â”‚    â”‚    â””â”€â”€â”€ ä½¿ç”¨MADæ›´ç¨³å¥
â”‚    â””â”€â”€â”€ IQRæ–¹æ³•
â”‚         â”œâ”€â”€â”€ Q1 - 1.5Ã—IQR
â”‚         â”œâ”€â”€â”€ Q3 + 1.5Ã—IQR
â”‚         â””â”€â”€â”€ ç®±çº¿å›¾å¯è§†åŒ–
â”‚
â”œâ”€â”€â”€ åŸºäºè·ç¦»
â”‚    â”œâ”€â”€â”€ k-NNè·ç¦»
â”‚    â”‚    â””â”€â”€â”€ åˆ°ç¬¬kä¸ªé‚»å±…çš„è·ç¦»
â”‚    â””â”€â”€â”€ è·ç¦»èšåˆ
â”‚
â”œâ”€â”€â”€ åŸºäºå¯†åº¦
â”‚    â”œâ”€â”€â”€ LOFï¼ˆå±€éƒ¨å¼‚å¸¸å› å­ï¼‰
â”‚    â”‚    â”œâ”€â”€â”€ å±€éƒ¨å¯è¾¾å¯†åº¦
â”‚    â”‚    â””â”€â”€â”€ LOF >> 1 = å¼‚å¸¸
â”‚    â”œâ”€â”€â”€ DBSCAN
â”‚    â”‚    â””â”€â”€â”€ å™ªå£°ç‚¹ = å¼‚å¸¸
â”‚    â””â”€â”€â”€ COF
â”‚
â”œâ”€â”€â”€ æœºå™¨å­¦ä¹ 
â”‚    â”œâ”€â”€â”€ éš”ç¦»æ£®æ—
â”‚    â”‚    â”œâ”€â”€â”€ éšæœºåˆ†è£‚
â”‚    â”‚    â”œâ”€â”€â”€ è·¯å¾„é•¿åº¦
â”‚    â”‚    â””â”€â”€â”€ é€‚åˆé«˜ç»´
â”‚    â”œâ”€â”€â”€ One-Class SVM
â”‚    â”‚    â”œâ”€â”€â”€ è¶…å¹³é¢/è¶…çƒ
â”‚    â”‚    â””â”€â”€â”€ nuå‚æ•°
â”‚    â””â”€â”€â”€ è‡ªåŠ¨ç¼–ç å™¨
â”‚         â”œâ”€â”€â”€ é‡å»ºè¯¯å·®
â”‚         â””â”€â”€â”€ æ·±åº¦å­¦ä¹ 
â”‚
â”œâ”€â”€â”€ å¤„ç†ç­–ç•¥
â”‚    â”œâ”€â”€â”€ åˆ é™¤
â”‚    â”œâ”€â”€â”€ æ›¿æ¢ï¼ˆå‡å€¼/ä¸­ä½æ•°ï¼‰
â”‚    â”œâ”€â”€â”€ è½¬æ¢ï¼ˆå¯¹æ•°/å¹³æ–¹æ ¹ï¼‰
â”‚    â”œâ”€â”€â”€ åˆ†ç®±
â”‚    â”œâ”€â”€â”€ ä¿ç•™
â”‚    â””â”€â”€â”€ å•ç‹¬å»ºæ¨¡
â”‚
â”œâ”€â”€â”€ åº”ç”¨åœºæ™¯
â”‚    â”œâ”€â”€â”€ æ¬ºè¯ˆæ£€æµ‹
â”‚    â”‚    â”œâ”€â”€â”€ ä¿¡ç”¨å¡
â”‚    â”‚    â””â”€â”€â”€ ä¿é™©ç†èµ”
â”‚    â”œâ”€â”€â”€ ç½‘ç»œå®‰å…¨
â”‚    â”‚    â””â”€â”€â”€ å…¥ä¾µæ£€æµ‹
â”‚    â”œâ”€â”€â”€ åŒ»ç–—
â”‚    â”‚    â””â”€â”€â”€ å¼‚å¸¸ç”Ÿå‘½ä½“å¾
â”‚    â”œâ”€â”€â”€ åˆ¶é€ ä¸š
â”‚    â”‚    â””â”€â”€â”€ è´¨é‡æ§åˆ¶
â”‚    â””â”€â”€â”€ ç‰©è”ç½‘
â”‚         â””â”€â”€â”€ ä¼ æ„Ÿå™¨æ•…éšœ
â”‚
â””â”€â”€â”€ è¯„ä¼°æŒ‡æ ‡
     â”œâ”€â”€â”€ ç²¾ç¡®ç‡
     â”œâ”€â”€â”€ å¬å›ç‡
     â”œâ”€â”€â”€ F1åˆ†æ•°
     â”œâ”€â”€â”€ ROC-AUC
     â””â”€â”€â”€ æ··æ·†çŸ©é˜µ
```

## ğŸ“– å­¦ä¹ èµ„æº

### è¯¾ç¨‹ææ–™

- **05_CST8502_OutlierDetection1.pdf**ï¼šå¼‚å¸¸å€¼æ£€æµ‹ç»¼åˆæŒ‡å—

### Python åº“

- **scikit-learn**ï¼šIsolationForest, LocalOutlierFactor, OneClassSVM
- **PyOD**ï¼šä¸“é—¨çš„å¼‚å¸¸æ£€æµ‹åº“
- **scipy.stats**ï¼šç»Ÿè®¡æ£€éªŒ
- **pandas**ï¼šæ•°æ®åˆ†æ

### æ¨èé˜…è¯»

- Chandola, V. et al. (2009). "Anomaly Detection: A Survey"
- Breunig, M. et al. (2000). "LOF: Identifying Density-Based Local Outliers"
- Liu, F. T. et al. (2008). "Isolation Forest"

## ğŸ¯ å­¦ä¹ ç›®æ ‡æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬ç« åï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿï¼š

- âœ… ç†è§£å¼‚å¸¸å€¼çš„ç±»å‹å’Œäº§ç”ŸåŸå› 
- âœ… åº”ç”¨ç»Ÿè®¡æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼ï¼ˆZ åˆ†æ•°ã€IQRï¼‰
- âœ… ä½¿ç”¨åŸºäºè·ç¦»å’Œå¯†åº¦çš„æ–¹æ³•ï¼ˆk-NNã€LOFã€DBSCANï¼‰
- âœ… å®ç°æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆéš”ç¦»æ£®æ—ã€One-Class SVMï¼‰
- âœ… é€‰æ‹©åˆé€‚çš„å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•
- âœ… å†³å®šå¦‚ä½•å¤„ç†æ£€æµ‹åˆ°çš„å¼‚å¸¸å€¼
- âœ… åº”ç”¨å¼‚å¸¸æ£€æµ‹åˆ°å®é™…é—®é¢˜ï¼ˆæ¬ºè¯ˆæ£€æµ‹ã€è´¨é‡æ§åˆ¶ï¼‰
- âœ… è¯„ä¼°å¼‚å¸¸æ£€æµ‹æ¨¡å‹çš„æ€§èƒ½
- âœ… å¯è§†åŒ–å¼‚å¸¸å€¼

---

**ä¸‹ä¸€ç« é¢„å‘Šï¼š** ç¬¬å…­ç« å°†å­¦ä¹ èšç±»ç®—æ³•ï¼Œç‰¹åˆ«æ˜¯ k-Means ç®—æ³•ï¼Œæ¢ç´¢æ— ç›‘ç£å­¦ä¹ çš„ä¸–ç•Œã€‚
