# CST8502 机器学习 - 综合考试题库

> **考试说明**：本题库覆盖所有章节内容，包含 25 道选择题、10 道填空题、简答题和 1 道决策树计算题。
>
> **总分**：50 分
>
> - 选择题：25 分（每题 1 分）
> - 填空题：10 分（每题 1 分）
> - 简答题：10 分
> - 决策树计算题：5 分

---

## 📝 第一部分：选择题（Multiple Choice Questions）

**说明**：每题 1 分，共 25 分。选择最佳答案。

### 第 1 题

**来源：第 1 章 - 机器学习导论**

机器学习的三大主要类型不包括以下哪一项？

- A. 监督学习
- B. 无监督学习
- C. 确定性学习
- D. 强化学习

<details>
<summary>查看答案</summary>

**答案：C**

**解释**：机器学习的三大类型是监督学习、无监督学习和强化学习。确定性学习不是机器学习的标准分类。

</details>

---

### 第 2 题

**来源：第 1 章 - 机器学习导论**

在监督学习中，垃圾邮件检测属于哪种问题类型？

- A. 回归问题
- B. 分类问题
- C. 聚类问题
- D. 降维问题

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：垃圾邮件检测需要将邮件分类为"垃圾邮件"或"正常邮件"，属于二分类问题。

**详细说明**：

- **分类问题**：输出是离散的类别标签（垃圾邮件/正常邮件）
- **监督学习**：需要大量标记好的训练数据来学习分类模式
- **二分类**：只有两个可能的输出结果
- **特征提取**：基于邮件内容、发件人、关键词等特征进行分类
- **应用场景**：邮件系统自动过滤垃圾邮件，提高用户体验

**其他选项分析**：

- A. 回归问题：预测连续数值，不适用于分类任务
- C. 聚类问题：无监督学习，不需要标记数据
- D. 降维问题：减少数据维度，不涉及分类

</details>

---

### 第 3 题

**来源：第 1 章 - 机器学习导论 & 第 2 章 - 数据预处理**

标准差（Standard Deviation）衡量的是数据的什么特征？

- A. 中心位置
- B. 分散程度
- C. 偏斜度
- D. 相关性

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：标准差和方差都用于衡量数据的离散程度或分散程度。

</details>

---

### 第 4 题

**来源：第 2 章 - 数据预处理与 k-NN**

Min-Max 归一化将数据缩放到什么范围？

- A. [-1, 1]
- B. [0, 1]
- C. [0, 100]
- D. 任意范围

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：Min-Max 归一化公式为 x' = (x - min) / (max - min)，将数据缩放到 [0, 1] 范围。

</details>

---

### 第 5 题

**来源：第 2 章 - 数据预处理与 k-NN**

k-NN 算法中，k 值越小会导致什么问题？

- A. 欠拟合
- B. 过拟合
- C. 计算速度慢
- D. 内存不足

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：k 值越小，模型越复杂，更容易受到噪声影响，导致过拟合。

</details>

---

### 第 6 题

**来源：第 2 章 - 数据预处理与 k-NN**

以下哪种距离度量不适用于 k-NN 算法？

- A. 欧几里得距离
- B. 曼哈顿距离
- C. 余弦相似度
- D. 以上都适用

<details>
<summary>查看答案</summary>

**答案：D**

**解释**：k-NN 可以使用多种距离度量，包括欧几里得距离、曼哈顿距离和余弦相似度。

</details>

---

### 第 7 题

**来源：第 3 章 - 分类与决策树**

决策树中，信息增益（Information Gain）基于什么概念？

- A. 方差
- B. 熵（Entropy）
- C. 均值
- D. 中位数

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：信息增益是基于熵的概念，衡量特征对减少不确定性的贡献。

</details>

---

### 第 8 题

**来源：第 3 章 - 分类与决策树**

ID3 算法使用什么标准选择分裂特征？

- A. 基尼不纯度
- B. 信息增益
- C. 方差减少
- D. 卡方统计量

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：ID3 算法使用信息增益作为特征选择标准。

</details>

---

### 第 9 题

**来源：第 3 章 - 分类与决策树**

以下哪个算法可以同时用于分类和回归问题？

- A. ID3
- B. C4.5
- C. CART
- D. 以上都可以

<details>
<summary>查看答案</summary>

**答案：C**

**解释**：CART（Classification and Regression Trees）算法可以用于分类和回归问题，而 ID3 和 C4.5 主要用于分类。

</details>

---

### 第 10 题

**来源：第 3 章 - 分类与决策树**

预剪枝（Pre-pruning）的主要优点是什么？

- A. 更准确
- B. 更快速
- C. 更复杂
- D. 更稳定

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：预剪枝在构建决策树的过程中提前停止分裂，因此更快速，节省计算资源。

</details>

---

### 第 11 题

**来源：第 5 章 - 异常值检测**

Z 分数法（Z-Score Method）假设数据服从什么分布？

- A. 均匀分布
- B. 正态分布
- C. 泊松分布
- D. 指数分布

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：Z 分数法基于正态分布假设，使用均值和标准差来识别异常值。

</details>

---

### 第 12 题

**来源：第 5 章 - 异常值检测**

IQR 方法中，通常认为超过 Q3 + 1.5×IQR 的点是什么？

- A. 正常值
- B. 异常值
- C. 缺失值
- D. 中位数

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：IQR 方法定义异常值为小于 Q1 - 1.5×IQR 或大于 Q3 + 1.5×IQR 的值。

</details>

---

### 第 13 题

**来源：第 5 章 - 异常值检测**

LOF（Local Outlier Factor）算法基于什么原理？

- A. 距离
- B. 密度
- C. 统计检验
- D. 聚类

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：LOF 算法基于密度原理，比较每个点的局部密度与其邻居的局部密度。

</details>

---

### 第 14 题

**来源：第 5 章 - 异常值检测**

隔离森林（Isolation Forest）中，异常值的特点是什么？

- A. 路径长度较长
- B. 路径长度较短
- C. 密度较高
- D. 距离较近

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：隔离森林基于异常值更容易被"隔离"的原理，因此异常值的平均路径长度较短。

</details>

---

### 第 15 题

**来源：第 5 章 - 异常值检测**

自动编码器（Autoencoder）检测异常值基于什么指标？

- A. 距离
- B. 密度
- C. 重建误差
- D. 分类概率

<details>
<summary>查看答案</summary>

**答案：C**

**解释**：自动编码器学习正常数据的压缩和重建，异常值的重建误差通常较大。

</details>

---

### 第 16 题

**来源：第 6 章 - 聚类与 k-Means 算法**

k-Means 算法的主要目标是什么？

- A. 最大化簇间距离
- B. 最小化簇内平方和（WCSS）
- C. 最大化轮廓系数
- D. 最小化簇数

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：k-Means 算法的目标是最小化簇内平方和（Within-Cluster Sum of Squares, WCSS）。

</details>

---

### 第 17 题

**来源：第 6 章 - 聚类与 k-Means 算法**

轮廓系数（Silhouette Score）的范围是多少？

- A. [0, 1]
- B. [-1, 1]
- C. [0, ∞)
- D. (-∞, +∞)

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：轮廓系数的范围是 [-1, 1]，接近 1 表示聚类良好，接近 -1 表示分配可能错误。

</details>

---

### 第 18 题

**来源：第 6 章 - 聚类与 k-Means 算法**

k-Means++ 算法主要改进了 k-Means 的哪个方面？

- A. 收敛速度
- B. 质心初始化
- C. 距离计算
- D. 簇数选择

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：k-Means++ 改进了质心的初始化方法，使质心更加分散，从而获得更好的聚类结果。

</details>

---

### 第 19 题

**来源：第 6 章 - 聚类与 k-Means 算法**

以下哪个不是 k-Means 算法的假设？

- A. 簇是球形的
- B. 簇大小相似
- C. 簇密度相似
- D. 簇可以重叠

<details>
<summary>查看答案</summary>

**答案：D**

**解释**：k-Means 假设簇是不重叠的、球形的、大小和密度相似的。

</details>

---

### 第 20 题

**来源：第 6 章 - 聚类与 k-Means 算法**

Mini-Batch k-Means 的主要优势是什么？

- A. 更准确
- B. 更快速
- C. 更稳定
- D. 自动确定簇数

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：Mini-Batch k-Means 使用小批量数据进行迭代，大大提高了计算速度，适合大数据集。

</details>

---

### 第 21 题

**来源：第 1 章 - 机器学习导论**

过拟合（Overfitting）的典型特征是什么？

- A. 训练集和测试集表现都差
- B. 训练集表现好，测试集表现差
- C. 训练集表现差，测试集表现好
- D. 训练集和测试集表现都好

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：过拟合是指模型在训练数据上表现很好，但在测试数据上表现差，失去了泛化能力。

</details>

---

### 第 22 题

**来源：第 1 章 - 机器学习导论**

交叉验证（Cross-Validation）的主要目的是什么？

- A. 加快训练速度
- B. 评估模型泛化能力
- C. 减少内存使用
- D. 简化模型

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：交叉验证通过在不同数据子集上训练和测试模型，来评估模型的泛化能力。

</details>

---

### 第 23 题

**来源：第 2 章 - 数据预处理与 k-NN**

以下哪种方法不是处理缺失值的常用方法？

- A. 删除含缺失值的行
- B. 用均值填充
- C. 用随机值填充
- D. 插值法

<details>
<summary>查看答案</summary>

**答案：C**

**解释**：用随机值填充不是常用的缺失值处理方法，因为会引入额外的噪声。常用方法包括删除、用均值/中位数/众数填充、插值等。

</details>

---

### 第 24 题

**来源：第 2 章 - 数据预处理与 k-NN**

独热编码（One-Hot Encoding）会对特征数量产生什么影响？

- A. 减少特征数量
- B. 增加特征数量
- C. 保持特征数量不变
- D. 删除特征

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：独热编码将一个分类特征转换为多个二进制特征，因此会增加特征数量。

</details>

---

### 第 25 题

**来源：第 2 章 - 数据预处理与 k-NN**

维度诅咒（Curse of Dimensionality）是指什么问题？

- A. 特征太少
- B. 数据太少
- C. 高维空间中距离失去意义
- D. 算法太复杂

<details>
<summary>查看答案</summary>

**答案：C**

**解释**：维度诅咒是指在高维空间中，距离度量变得不再有效，所有点之间的距离趋于相似。

</details>

---

## 📝 第二部分：填空题（One-Word Questions）

**说明**：每题 1 分，共 10 分。用一个词或简短短语填空。

### 第 26 题

**来源：第 1 章 - 机器学习导论**

机器学习中，使用标记数据进行训练的学习方式称为 \***\*\_\_\*\*** 学习。

<details>
<summary>查看答案</summary>

**答案**：监督（Supervised）

</details>

---

### 第 27 题

**来源：第 2 章 - 数据预处理与 k-NN**

标准化（Z-score Normalization）后的数据，均值为 0，标准差为 \***\*\_\_\*\***。

<details>
<summary>查看答案</summary>

**答案**：1

</details>

---

### 第 28 题

**来源：第 2 章 - 数据预处理与 k-NN**

k-NN 算法中的 "k" 表示最近 \***\*\_\_\*\*** 的数量。

<details>
<summary>查看答案</summary>

**答案**：邻居（neighbors）

</details>

---

### 第 29 题

**来源：第 3 章 - 分类与决策树**

决策树中，熵为 0 表示节点是 \***\*\_\_\*\*** 的（完全纯净）。

<details>
<summary>查看答案</summary>

**答案**：纯净（pure）

</details>

---

### 第 30 题

**来源：第 3 章 - 分类与决策树**

CART 算法使用 \***\*\_\_\*\*** 不纯度作为分裂标准。

<details>
<summary>查看答案</summary>

**答案**：基尼（Gini）

</details>

---

### 第 31 题

**来源：第 5 章 - 异常值检测**

IQR 是第三四分位数（Q3）减去第一四分位数（Q1），代表 \***\*\_\_\*\*** 范围。

<details>
<summary>查看答案</summary>

**答案**：四分位距（Interquartile Range）或 中间 50%

</details>

---

### 第 32 题

**来源：第 5 章 - 异常值检测**

LOF 算法中，LOF 值远大于 1 表示该点是 \***\*\_\_\*\***。

<details>
<summary>查看答案</summary>

**答案**：异常值（outlier）

</details>

---

### 第 33 题

**来源：第 5 章 - 异常值检测**

隔离森林算法基于 \***\*\_\_\*\*** 决策树的集成。

<details>
<summary>查看答案</summary>

**答案**：随机（random）

</details>

---

### 第 34 题

**来源：第 6 章 - 聚类与 k-Means 算法**

k-Means 算法中，簇的中心点称为 \***\*\_\_\*\***。

<details>
<summary>查看答案</summary>

**答案**：质心（centroid）

</details>

---

### 第 35 题

**来源：第 6 章 - 聚类与 k-Means 算法**

肘部法（Elbow Method）通过绘制 \***\*\_\_\*\*** 与 k 值的关系来选择最优簇数。

<details>
<summary>查看答案</summary>

**答案**：WCSS 或 簇内平方和（Within-Cluster Sum of Squares）

</details>

---

## 📝 第三部分：简答题（Short Answer Questions）

**说明**：共 10 分。请简明扼要地回答以下问题。

### 第 36 题（3 分）

**来源：第 1 章 - 机器学习导论**

解释监督学习和无监督学习的主要区别，并各举一个应用实例。

<details>
<summary>查看参考答案</summary>

**参考答案**：

**主要区别**（1.5 分）：

- 监督学习使用标记数据（有输入和对应的正确输出），目标是学习输入到输出的映射关系。
- 无监督学习使用未标记数据，目标是发现数据中的隐藏模式和结构。

**应用实例**（1.5 分）：

- 监督学习：垃圾邮件检测、房价预测、图像分类
- 无监督学习：客户细分、异常检测、主题发现

**评分要点**：

- 清楚说明数据标记的区别（1 分）
- 说明学习目标的区别（0.5 分）
- 每个类型至少一个正确实例（1.5 分）
</details>

---

### 第 37 题（4 分）

**来源：第 2 章 - 数据预处理与 k-NN**

k-NN 算法有哪些优点和缺点？并说明如何选择合适的 k 值。

<details>
<summary>查看参考答案</summary>

**参考答案**：

**优点**（1.5 分）：

- 简单易懂，易于实现
- 无需训练过程（懒惰学习）
- 对非线性数据效果好
- 可用于分类和回归

**缺点**（1.5 分）：

- 计算成本高（需要计算到所有点的距离）
- 内存需求大（需要存储所有训练数据）
- 对特征缩放敏感
- 受维度诅咒影响

**选择 k 值**（1 分）：

- 使用交叉验证评估不同 k 值的性能
- 绘制 k 值与性能的曲线
- 考虑奇数 k 值避免平局
- 经验法则：k ≈ √n

**评分要点**：

- 至少列出 3 个优点（1.5 分）
- 至少列出 3 个缺点（1.5 分）
- 说明 k 值选择方法（1 分）
</details>

---

### 第 38 题（3 分）

**来源：第 1 章 - 机器学习导论 & 第 3 章 - 决策树**

什么是过拟合？如何避免决策树的过拟合问题？

<details>
<summary>查看参考答案</summary>

**参考答案**：

**过拟合定义**（1 分）：
过拟合是指模型在训练数据上表现很好，但在新数据（测试数据）上表现差，因为模型学习了训练数据中的噪声和特殊细节，失去了泛化能力。

**避免方法**（2 分）：

- **预剪枝**：限制最大深度、最小分裂样本数、最小叶节点样本数
- **后剪枝**：先构建完整树再删除不必要的分支
- **使用验证集**：评估模型在独立数据上的性能
- **集成方法**：使用随机森林、梯度提升等
- **正则化**：成本复杂度剪枝

**评分要点**：

- 清楚定义过拟合（1 分）
- 至少列出 3 种避免方法（2 分）
</details>

---

## 📝 第四部分：决策树计算题（Decision Tree Math Question）

**说明**：5 分。展示完整的计算过程。

### 第 39 题（5 分）

**来源：第 3 章 - 分类与决策树**

给定以下天气数据集，使用 ID3 算法（基于信息增益）构建决策树的第一层（只需确定根节点）。

**数据集：**

| 序号 | 天气 | 温度 | 湿度 | 风力 | 打球 |
| ---- | ---- | ---- | ---- | ---- | ---- |
| 1    | 晴   | 热   | 高   | 弱   | 否   |
| 2    | 晴   | 热   | 高   | 强   | 否   |
| 3    | 阴   | 热   | 高   | 弱   | 是   |
| 4    | 雨   | 温和 | 高   | 弱   | 是   |
| 5    | 雨   | 凉爽 | 正常 | 弱   | 是   |
| 6    | 雨   | 凉爽 | 正常 | 强   | 否   |
| 7    | 阴   | 凉爽 | 正常 | 强   | 是   |
| 8    | 晴   | 温和 | 高   | 弱   | 否   |
| 9    | 晴   | 凉爽 | 正常 | 弱   | 是   |
| 10   | 雨   | 温和 | 正常 | 弱   | 是   |

**要求**：

1. 计算目标变量"打球"的熵 H(Play)（1 分）
2. 计算特征"天气"的信息增益 IG(Play, 天气)（2 分）
3. 计算特征"湿度"的信息增益 IG(Play, 湿度)（2 分）
4. 确定根节点应该选择哪个特征（不需要计算其他特征）

**公式提示**：

- 熵：$H(S) = -\sum_{i=1}^{c} p_i \log_2(p_i)$
- 信息增益：$IG(S, A) = H(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} H(S_v)$

<details>
<summary>查看详细解答</summary>

**完整解答：**

---

**步骤 1：计算目标变量的熵 H(Play)**（1 分）

统计数据：

- 总样本数：10
- 打球"是"：6 个（样本 3, 4, 5, 7, 9, 10）
- 打球"否"：4 个（样本 1, 2, 6, 8）

计算熵：
$$H(Play) = -\frac{6}{10}\log_2(\frac{6}{10}) - \frac{4}{10}\log_2(\frac{4}{10})$$

$$= -0.6 \times (-0.737) - 0.4 \times (-1.322)$$

$$= 0.442 + 0.529 = 0.971$$

**H(Play) = 0.971**

---

**步骤 2：计算特征"天气"的信息增益**（2 分）

**统计"天气"特征的分布：**

- **晴天**：4 个样本（1, 2, 8, 9）

  - 是：1 个（9）
  - 否：3 个（1, 2, 8）
  - $H(\text{晴}) = -\frac{1}{4}\log_2(\frac{1}{4}) - \frac{3}{4}\log_2(\frac{3}{4})$
  - $= -0.25 \times (-2) - 0.75 \times (-0.415)$
  - $= 0.5 + 0.311 = 0.811$

- **阴天**：2 个样本（3, 7）

  - 是：2 个（3, 7）
  - 否：0 个
  - $H(\text{阴}) = 0$（纯净！）

- **雨天**：4 个样本（4, 5, 6, 10）
  - 是：3 个（4, 5, 10）
  - 否：1 个（6）
  - $H(\text{雨}) = -\frac{3}{4}\log_2(\frac{3}{4}) - \frac{1}{4}\log_2(\frac{1}{4})$
  - $= -0.75 \times (-0.415) - 0.25 \times (-2)$
  - $= 0.311 + 0.5 = 0.811$

**计算加权平均熵：**
$$H_{\text{天气}} = \frac{4}{10} \times 0.811 + \frac{2}{10} \times 0 + \frac{4}{10} \times 0.811$$
$$= 0.324 + 0 + 0.324 = 0.648$$

**计算信息增益：**
$$IG(Play, \text{天气}) = 0.971 - 0.648 = 0.323$$

**IG(Play, 天气) = 0.323**

---

**步骤 3：计算特征"湿度"的信息增益**（2 分）

**统计"湿度"特征的分布：**

- **高湿度**：6 个样本（1, 2, 3, 4, 8）

  - 是：2 个（3, 4）
  - 否：3 个（1, 2, 8）
  - $H(\text{高}) = -\frac{2}{5}\log_2(\frac{2}{5}) - \frac{3}{5}\log_2(\frac{3}{5})$
  - $= -0.4 \times (-1.322) - 0.6 \times (-0.737)$
  - $= 0.529 + 0.442 = 0.971$

- **正常湿度**：5 个样本（5, 6, 7, 9, 10）
  - 是：4 个（5, 7, 9, 10）
  - 否：1 个（6）
  - $H(\text{正常}) = -\frac{4}{5}\log_2(\frac{4}{5}) - \frac{1}{5}\log_2(\frac{1}{5})$
  - $= -0.8 \times (-0.322) - 0.2 \times (-2.322)$
  - $= 0.258 + 0.464 = 0.722$

**计算加权平均熵：**
$$H_{\text{湿度}} = \frac{5}{10} \times 0.971 + \frac{5}{10} \times 0.722$$
$$= 0.486 + 0.361 = 0.847$$

**计算信息增益：**
$$IG(Play, \text{湿度}) = 0.971 - 0.847 = 0.124$$

**IG(Play, 湿度) = 0.124**

---

**步骤 4：确定根节点**

比较信息增益：

- **IG(Play, 天气) = 0.323** ⭐（最大）
- IG(Play, 湿度) = 0.124

**结论：根节点应该选择"天气"特征**，因为它具有最大的信息增益。

---

**评分标准**：

- 正确计算 H(Play)（1 分）
- 正确计算 IG(Play, 天气)，包括中间步骤（2 分）
  - 正确统计各子集（0.5 分）
  - 正确计算各子集熵（1 分）
  - 正确计算信息增益（0.5 分）
- 正确计算 IG(Play, 湿度)，包括中间步骤（2 分）
  - 正确统计各子集（0.5 分）
  - 正确计算各子集熵（1 分）
  - 正确计算信息增益（0.5 分）
- 部分分：如果计算方法正确但算术错误，可得 80% 分数

</details>

---

## 🎓 考试结束

**总分：50 分**

- 选择题（1-25）：25 分
- 填空题（26-35）：10 分
- 简答题（36-38）：10 分
- 决策树计算题（39）：5 分

---

## 📊 评分标准

### A 级（45-50 分）

- 优秀掌握所有核心概念
- 能够正确应用算法
- 计算题步骤完整，结果准确

### B 级（40-44 分）

- 良好掌握大部分概念
- 基本能够应用算法
- 计算题方法正确，可能有小错误

### C 级（35-39 分）

- 掌握基本概念
- 能够部分应用算法
- 计算题理解基本方法

### D 级（30-34 分）

- 了解部分基本概念
- 应用能力较弱
- 计算题有明显困难

### F 级（<30 分）

- 基本概念不清楚
- 无法应用算法
- 计算题无法完成

---

## 📚 复习建议

### 重点复习内容

1. **第 1 章：机器学习导论**

   - 三大学习类型的区别
   - 过拟合和欠拟合
   - 基本统计概念

2. **第 2 章：数据预处理与 k-NN**

   - 归一化和标准化的公式和应用
   - k-NN 算法原理和 k 值选择
   - 距离度量方法

3. **第 3 章：决策树**

   - 熵和信息增益的计算（重点！）
   - ID3、C4.5、CART 的区别
   - 剪枝技术

4. **第 5 章：异常值检测**

   - Z 分数法和 IQR 方法
   - LOF 和隔离森林原理
   - 异常值处理策略

5. **第 6 章：聚类**
   - k-Means 算法步骤
   - WCSS 和轮廓系数
   - k 值选择方法

### 练习建议

- 多做手工计算题，特别是决策树的熵和信息增益
- 理解每个算法的优缺点和适用场景
- 能够比较不同算法的特点
- 掌握评估指标的含义和计算

---

**祝考试顺利！Good Luck! 🍀**
