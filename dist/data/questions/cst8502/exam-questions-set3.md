# CST8502 机器学习 - 综合考试题库（第三套）

> **考试说明**：本题库覆盖所有章节内容，包含 25 道选择题、10 道填空题、简答题、1 道 KNN 计算题和 1 道模型评估计算题。
>
> **总分**：55 分
>
> - 选择题：25 分（每题 1 分）
> - 填空题：10 分（每题 1 分）
> - 简答题：10 分
> - KNN 算法计算题：5 分
> - 模型评估计算题（ROC 曲线）：5 分

---

## 📝 第一部分：选择题（Multiple Choice Questions）

**说明**：每题 1 分，共 25 分。选择最佳答案。

### 第 1 题

**来源：第 1 章 - 机器学习导论**

机器学习算法从数据中学习的本质是什么？

- A. 记忆所有训练数据
- B. 发现数据中的模式和规律
- C. 随机生成预测结果
- D. 复制人类的思维过程

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：机器学习的核心是从数据中发现模式和规律，并利用这些模式对新数据进行预测或决策。不是简单的记忆或随机生成。

</details>

---

### 第 2 题

**来源：第 1 章 - 机器学习导论**

以下哪个应用场景最适合使用无监督学习？

- A. 手写数字识别
- B. 房价预测
- C. 客户市场细分
- D. 垃圾邮件过滤

<details>
<summary>查看答案</summary>

**答案：C**

**解释**：客户市场细分是典型的聚类问题，属于无监督学习。其他选项都需要标记数据，属于监督学习。

</details>

---

### 第 3 题

**来源：第 1 章 - 机器学习导论**

训练集、验证集和测试集的典型比例是？

- A. 50% / 25% / 25%
- B. 70% / 15% / 15%
- C. 80% / 10% / 10%
- D. 60% / 20% / 20%

<details>
<summary>查看答案</summary>

**答案：D**

**解释**：常见的数据集划分比例是 60% 训练集、20% 验证集、20% 测试集。也可以使用 70/15/15 或 80/10/10，但 60/20/20 更为常见且平衡。

</details>

---

### 第 4 题

**来源：第 2 章 - Python 基础**

NumPy 数组相比 Python 列表的主要优势是什么？

- A. 更容易理解
- B. 计算速度更快
- C. 占用内存更多
- D. 支持更多数据类型

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：NumPy 数组使用连续的内存块和向量化操作，计算速度远快于 Python 列表。

</details>

---

### 第 5 题

**来源：第 2 章 - Python 基础**

Pandas 中用于处理缺失值的方法是？

- A. `fillna()` 和 `dropna()`
- B. `remove()` 和 `add()`
- C. `delete()` 和 `insert()`
- D. `clear()` 和 `set()`

<details>
<summary>查看答案</summary>

**答案：A**

**解释**：`fillna()` 用于填充缺失值，`dropna()` 用于删除包含缺失值的行或列。

</details>

---

### 第 6 题

**来源：第 3 章 - 线性回归**

线性回归的目标是最小化什么？

- A. 预测值的总和
- B. 残差平方和（RSS）
- C. 特征的数量
- D. 样本的数量

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：线性回归通过最小化残差平方和（RSS）或均方误差（MSE）来找到最佳拟合线。

</details>

---

### 第 7 题

**来源：第 3 章 - 线性回归**

多重共线性会导致什么问题？

- A. 模型训练速度加快
- B. 系数估计不稳定
- C. 预测精度提高
- D. 数据量增加

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：多重共线性使得特征之间高度相关，导致回归系数估计不稳定，难以解释每个特征的独立影响。

</details>

---

### 第 8 题

**来源：第 3 章 - 线性回归**

Ridge 回归（岭回归）使用什么正则化方法？

- A. L0 正则化
- B. L1 正则化
- C. L2 正则化
- D. L3 正则化

<details>
<summary>查看答案</summary>

**答案：C**

**解释**：Ridge 回归使用 L2 正则化（系数平方和），而 Lasso 回归使用 L1 正则化（系数绝对值和）。

</details>

---

### 第 9 题

**来源：第 4 章 - 逻辑回归**

逻辑回归使用什么函数将线性输出转换为概率？

- A. ReLU 函数
- B. Sigmoid 函数
- C. Tanh 函数
- D. Softmax 函数

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：逻辑回归使用 Sigmoid 函数（也称为 Logistic 函数）将线性输出压缩到 0-1 之间，表示概率。

</details>

---

### 第 10 题

**来源：第 4 章 - 逻辑回归**

在二分类问题中，如果提高分类阈值（threshold），会发生什么？

- A. 精确率降低，召回率提高
- B. 精确率提高，召回率降低
- C. 精确率和召回率都提高
- D. 精确率和召回率都降低

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：提高阈值意味着更严格的正类判断标准，会减少假阳性（提高精确率），但也会增加假阴性（降低召回率）。

</details>

---

### 第 11 题

**来源：第 4 章 - 逻辑回归**

多分类逻辑回归通常使用什么技术？

- A. One-vs-Rest (OvR)
- B. K-means
- C. PCA
- D. Decision Boundary

<details>
<summary>查看答案</summary>

**答案：A**

**解释**：多分类问题可以使用 One-vs-Rest（一对多）或 Softmax 回归来解决。

</details>

---

### 第 12 题

**来源：第 5 章 - 决策树**

决策树的哪个参数用于控制树的最大深度？

- A. `n_estimators`
- B. `max_depth`
- C. `learning_rate`
- D. `n_neighbors`

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：`max_depth` 参数限制决策树的最大深度，用于防止过拟合。

</details>

---

### 第 13 题

**来源：第 5 章 - 决策树**

信息增益基于什么度量？

- A. 均方误差
- B. 熵（Entropy）
- C. 准确率
- D. 召回率

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：信息增益基于熵的概念，衡量特征对于数据集不确定性的减少程度。

</details>

---

### 第 14 题

**来源：第 5 章 - 决策树**

以下哪个不是决策树剪枝的目的？

- A. 防止过拟合
- B. 提高泛化能力
- C. 增加训练速度
- D. 简化模型

<details>
<summary>查看答案</summary>

**答案：C**

**解释**：剪枝的主要目的是防止过拟合、提高泛化能力和简化模型，而不是增加训练速度。

</details>

---

### 第 15 题

**来源：第 6 章 - 随机森林**

随机森林属于什么类型的学习方法？

- A. 单一模型
- B. 集成学习
- C. 深度学习
- D. 强化学习

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：随机森林是集成学习方法，通过组合多个决策树来提高预测性能。

</details>

---

### 第 16 题

**来源：第 6 章 - 随机森林**

随机森林中的 "随机" 体现在哪两个方面？

- A. 随机样本和随机特征
- B. 随机权重和随机偏置
- C. 随机深度和随机宽度
- D. 随机训练和随机测试

<details>
<summary>查看答案</summary>

**答案：A**

**解释**：随机森林的随机性来自：1) Bootstrap 采样（随机样本），2) 每次分裂时随机选择特征子集。

</details>

---

### 第 17 题

**来源：第 6 章 - 随机森林**

袋装法（Bagging）的主要作用是什么？

- A. 增加模型复杂度
- B. 减少方差，防止过拟合
- C. 增加偏差
- D. 加快训练速度

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：Bagging 通过训练多个模型并平均预测结果，可以减少方差，提高模型稳定性。

</details>

---

### 第 18 题

**来源：第 7 章 - K 近邻算法**

KNN 是一种什么类型的学习算法？

- A. 参数化模型
- B. 非参数化模型
- C. 概率模型
- D. 线性模型

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：KNN 是非参数化模型，不需要训练参数，而是在预测时直接使用训练数据。

</details>

---

### 第 19 题

**来源：第 7 章 - K 近邻算法**

在 KNN 中，K 值的选择会如何影响模型？

- A. K 越大，模型越复杂
- B. K 越小，决策边界越平滑
- C. K 越大，模型对噪声越鲁棒
- D. K 值不影响模型性能

<details>
<summary>查看答案</summary>

**答案：C**

**解释**：较大的 K 值会使决策边界更平滑，对噪声更鲁棒，但可能导致欠拟合。较小的 K 值更敏感，容易过拟合。

</details>

---

### 第 20 题

**来源：第 7 章 - K 近邻算法**

为什么在使用 KNN 之前需要特征缩放？

- A. 加快训练速度
- B. 防止距离计算被大数值特征主导
- C. 减少内存使用
- D. 提高可解释性

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：KNN 基于距离计算，如果特征尺度差异大，大数值特征会主导距离计算，因此需要标准化。

</details>

---

### 第 21 题

**来源：第 8 章 - 支持向量机**

支持向量机的目标是什么？

- A. 最小化分类错误
- B. 最大化分类间隔
- C. 最小化特征数量
- D. 最大化样本数量

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：SVM 的目标是找到最大间隔超平面，使得两类之间的间隔最大化。

</details>

---

### 第 22 题

**来源：第 8 章 - 支持向量机**

核函数（Kernel）在 SVM 中的作用是什么？

- A. 加快训练速度
- B. 将数据映射到高维空间
- C. 减少特征数量
- D. 降低模型复杂度

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：核函数（如 RBF）将数据隐式映射到高维空间，使得线性不可分的数据在高维空间中变得线性可分。

</details>

---

### 第 23 题

**来源：第 9 章 - 聚类分析**

K-means 算法的目标是最小化什么？

- A. 类间距离
- B. 类内平方和（WCSS）
- C. 样本总数
- D. 特征数量

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：K-means 目标是最小化簇内平方和（Within-Cluster Sum of Squares, WCSS），使每个簇内部尽可能紧密。

</details>

---

### 第 24 题

**来源：第 9 章 - 聚类分析**

层次聚类（Hierarchical Clustering）的优势是什么？

- A. 不需要预先指定簇的数量
- B. 计算速度最快
- C. 只能处理数值数据
- D. 内存使用最少

<details>
<summary>查看答案</summary>

**答案：A**

**解释**：层次聚类生成树状图（Dendrogram），可以通过切割树来选择任意数量的簇，不需要预先指定。

</details>

---

### 第 25 题

**来源：第 10 章 - 降维技术**

PCA 的主要目标是什么？

- A. 增加数据维度
- B. 保留最大方差的方向
- C. 提高分类准确率
- D. 生成新的特征

<details>
<summary>查看答案</summary>

**答案：B**

**解释**：PCA（主成分分析）通过线性变换将数据投影到方差最大的方向（主成分），从而降低维度。

</details>

---

## 📝 第二部分：填空题（Fill in the Blanks）

**说明**：每题 1 分，共 10 分。

### 第 1 题

**来源：第 1 章**

机器学习可以分为三大类：**\_\_**、无监督学习和强化学习。

<details>
<summary>查看答案</summary>

**答案**：监督学习

</details>

---

### 第 2 题

**来源：第 2 章**

在 Pandas 中，**\_\_**是用于存储二维表格数据的主要数据结构。

<details>
<summary>查看答案</summary>

**答案**：DataFrame

</details>

---

### 第 3 题

**来源：第 3 章**

线性回归中，**\_\_**是衡量模型拟合优度的常用指标，取值范围是 0 到 1。

<details>
<summary>查看答案</summary>

**答案**：R²（R-squared / 决定系数）

</details>

---

### 第 4 题

**来源：第 4 章**

逻辑回归的损失函数称为**\_\_**，也称为对数损失。

<details>
<summary>查看答案</summary>

**答案**：交叉熵损失（Cross-Entropy Loss）/ Log Loss

</details>

---

### 第 5 题

**来源：第 5 章**

决策树中，用于衡量节点纯度的两个常用指标是基尼不纯度和**\_\_**。

<details>
<summary>查看答案</summary>

**答案**：熵（Entropy）/ 信息熵

</details>

---

### 第 6 题

**来源：第 6 章**

随机森林使用**\_\_**技术从原始数据集中有放回地抽取样本。

<details>
<summary>查看答案</summary>

**答案**：Bootstrap / 自助采样

</details>

---

### 第 7 题

**来源：第 7 章**

KNN 算法中，常用的距离度量包括欧几里得距离、曼哈顿距离和**\_\_**距离。

<details>
<summary>查看答案</summary>

**答案**：闵可夫斯基（Minkowski）距离 / 余弦（Cosine）距离

</details>

---

### 第 8 题

**来源：第 8 章**

在 SVM 中，离决策边界最近的训练样本称为**\_\_**。

<details>
<summary>查看答案</summary>

**答案**：支持向量（Support Vectors）

</details>

---

### 第 9 题

**来源：第 9 章**

**\_\_**是一种用于确定 K-means 最优簇数的方法，通过绘制 WCSS 随 K 值变化的曲线。

<details>
<summary>查看答案</summary>

**答案**：肘部法则（Elbow Method）

</details>

---

### 第 10 题

**来源：第 10 章**

t-SNE 是一种**\_\_**降维技术，主要用于高维数据的可视化。

<details>
<summary>查看答案</summary>

**答案**：非线性

</details>

---

## 📝 第三部分：简答题（Short Answer Questions）

**说明**：共 10 分。请简洁清晰地回答问题。

### 第 1 题（5 分）

**来源：综合应用**

**问题**：请解释偏差-方差权衡（Bias-Variance Tradeoff）的概念，并说明如何在实际应用中平衡这两者？

<details>
<summary>查看参考答案</summary>

**参考答案**：

**偏差-方差权衡的概念**（3 分）：

1. **偏差（Bias）**：模型预测值与真实值之间的差异，反映模型的欠拟合程度

   - 高偏差：模型过于简单，无法捕捉数据的真实模式（欠拟合）

2. **方差（Variance）**：模型在不同训练数据集上预测值的变化程度，反映模型的过拟合程度

   - 高方差：模型过于复杂，对训练数据过度敏感（过拟合）

3. **权衡关系**：降低偏差通常会增加方差，反之亦然

**平衡方法**（2 分）：

1. **模型复杂度调整**：

   - 选择适当复杂度的模型
   - 使用交叉验证选择最佳模型

2. **正则化**：

   - L1/L2 正则化控制模型复杂度
   - 决策树剪枝

3. **集成方法**：

   - Bagging 降低方差
   - Boosting 降低偏差

4. **特征工程**：
   - 添加有意义的特征降低偏差
   - 移除冗余特征降低方差

</details>

---

### 第 2 题（5 分）

**来源：综合应用**

**问题**：比较随机森林和梯度提升树（Gradient Boosting）的优缺点，并说明在什么情况下应该选择哪种算法？

<details>
<summary>查看参考答案</summary>

**参考答案**：

**随机森林**（2.5 分）：

**优点**：

- 并行训练，速度快
- 对噪声和异常值鲁棒
- 不容易过拟合
- 无需精细调参

**缺点**：

- 模型较大，内存占用多
- 预测性能可能不如 Boosting
- 难以解释

**梯度提升树**（2.5 分）：

**优点**：

- 预测精度通常更高
- 可以处理各种损失函数
- 特征重要性更可靠

**缺点**：

- 串行训练，速度慢
- 容易过拟合，需要仔细调参
- 对噪声敏感
- 训练时间长

**选择建议**：

**选择随机森林**：

- 数据有噪声和异常值
- 需要快速训练
- 计算资源有限
- 对精度要求不是特别高

**选择梯度提升**：

- 需要最高预测精度
- 数据质量较好
- 有足够的调参时间
- 可以容忍较长的训练时间

</details>

---

## 📝 第四部分：计算题（Computational Problems）

### 第 1 题：KNN 算法计算（5 分）

**来源：第 7 章 - K 近邻算法**

**问题**：

给定以下训练数据集，使用 KNN 算法（K=3）和欧几里得距离预测新样本 X = (6, 5) 的类别。

| 样本 | 特征 1 | 特征 2 | 类别 |
| ---- | ------ | ------ | ---- |
| A    | 2      | 3      | 红   |
| B    | 4      | 5      | 红   |
| C    | 3      | 2      | 蓝   |
| D    | 7      | 6      | 蓝   |
| E    | 8      | 7      | 蓝   |
| F    | 5      | 4      | 红   |

请计算：

1. 新样本 X 到每个训练样本的欧几里得距离（2 分）
2. 找出 K=3 个最近邻（1 分）
3. 预测 X 的类别（1 分）
4. 如果 K=5，预测结果会改变吗？为什么？（1 分）

<details>
<summary>查看详细解答</summary>

**解答**：

**步骤 1：计算欧几里得距离**（2 分）

欧几里得距离公式：$d = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$

新样本 X = (6, 5)

- d(X, A) = √[(6-2)² + (5-3)²] = √[16 + 4] = √20 = **4.47**
- d(X, B) = √[(6-4)² + (5-5)²] = √[4 + 0] = √4 = **2.00**
- d(X, C) = √[(6-3)² + (5-2)²] = √[9 + 9] = √18 = **4.24**
- d(X, D) = √[(6-7)² + (5-6)²] = √[1 + 1] = √2 = **1.41**
- d(X, E) = √[(6-8)² + (5-7)²] = √[4 + 4] = √8 = **2.83**
- d(X, F) = √[(6-5)² + (5-4)²] = √[1 + 1] = √2 = **1.41**

**步骤 2：找出 K=3 个最近邻**（1 分）

按距离排序：

1. D: 1.41（蓝）
2. F: 1.41（红）
3. B: 2.00（红）
4. E: 2.83（蓝）
5. C: 4.24（蓝）
6. A: 4.47（红）

K=3 的最近邻：D（蓝）、F（红）、B（红）

**步骤 3：预测类别**（1 分）

投票结果：

- 红色：2 票（F, B）
- 蓝色：1 票（D）

**预测结果：红色**

**步骤 4：如果 K=5 的情况**（1 分）

K=5 的最近邻：D（蓝）、F（红）、B（红）、E（蓝）、C（蓝）

投票结果：

- 红色：2 票
- 蓝色：3 票

**K=5 时预测结果：蓝色**

**结论**：是的，K=5 时预测结果会改变为蓝色。这说明 K 值的选择会影响预测结果，需要通过交叉验证选择最优 K 值。

</details>

---

### 第 2 题：模型评估计算（ROC 曲线）（5 分）

**来源：模型评估**

**问题**：

某二分类模型对 10 个测试样本的预测概率如下表所示：

| 样本 | 真实标签 | 预测概率 |
| ---- | -------- | -------- |
| 1    | 正       | 0.90     |
| 2    | 正       | 0.85     |
| 3    | 负       | 0.75     |
| 4    | 正       | 0.70     |
| 5    | 负       | 0.60     |
| 6    | 正       | 0.55     |
| 7    | 负       | 0.50     |
| 8    | 负       | 0.40     |
| 9    | 正       | 0.35     |
| 10   | 负       | 0.20     |

请完成：

1. 当阈值为 0.60 时，计算混淆矩阵（1 分）
2. 计算该阈值下的 TPR（真正率）和 FPR（假正率）（2 分）
3. 如果阈值改为 0.50，TPR 和 FPR 如何变化？（1 分）
4. 说明 ROC 曲线的含义及 AUC 的范围（1 分）

<details>
<summary>查看详细解答</summary>

**解答**：

**步骤 1：阈值 0.60 时的混淆矩阵**（1 分）

预测为正类（概率 ≥ 0.60）：样本 1, 2, 3, 4, 5
预测为负类（概率 < 0.60）：样本 6, 7, 8, 9, 10

真实情况：

- 正类：样本 1, 2, 4, 6, 9
- 负类：样本 3, 5, 7, 8, 10

混淆矩阵：

|              | 预测正类 | 预测负类 |
| ------------ | -------- | -------- |
| **实际正类** | 3 (TP)   | 2 (FN)   |
| **实际负类** | 2 (FP)   | 3 (TN)   |

- TP（真阳性）= 3（样本 1, 2, 4）
- FP（假阳性）= 2（样本 3, 5）
- FN（假阴性）= 2（样本 6, 9）
- TN（真阴性）= 3（样本 7, 8, 10）

**步骤 2：计算 TPR 和 FPR**（2 分）

**TPR（真正率 / 召回率 / 灵敏度）**：
$$TPR = \frac{TP}{TP + FN} = \frac{3}{3 + 2} = \frac{3}{5} = 0.60$$

**FPR（假正率）**：
$$FPR = \frac{FP}{FP + TN} = \frac{2}{2 + 3} = \frac{2}{5} = 0.40$$

**步骤 3：阈值改为 0.50 时的变化**（1 分）

阈值 0.50 时：

- 预测为正类：样本 1, 2, 3, 4, 5, 6, 7
- 预测为负类：样本 8, 9, 10

新的混淆矩阵：

- TP = 4（样本 1, 2, 4, 6）
- FP = 3（样本 3, 5, 7）
- FN = 1（样本 9）
- TN = 2（样本 8, 10）

新的指标：

- TPR = 4/(4+1) = 0.80（提高了）
- FPR = 3/(3+2) = 0.60（提高了）

**结论**：降低阈值会使更多样本被预测为正类，TPR 和 FPR 都会提高。

**步骤 4：ROC 曲线和 AUC**（1 分）

**ROC 曲线（Receiver Operating Characteristic Curve）**：

- 横轴：FPR（假正率）
- 纵轴：TPR（真正率）
- 通过改变分类阈值，绘制不同 (FPR, TPR) 点的曲线

**AUC（Area Under Curve）**：

- 表示 ROC 曲线下的面积
- 取值范围：**0.5 到 1.0**
- AUC = 0.5：随机猜测
- AUC = 1.0：完美分类器
- AUC > 0.8：通常认为是好的模型

**意义**：ROC 曲线和 AUC 提供了与阈值无关的模型性能评估，适合类别不平衡的情况。

</details>

---

## 📚 考试建议

1. **理解概念**：不要死记硬背，理解每个算法的原理和适用场景
2. **动手实践**：多练习编程实现，熟悉 scikit-learn 库的使用
3. **计算练习**：熟练掌握距离计算、决策树构建、混淆矩阵等计算
4. **对比总结**：建立算法对比表，了解各算法的优缺点
5. **关注细节**：注意参数含义、评估指标的计算公式

## 🎯 重点章节

- ⭐⭐⭐ 第 4 章（逻辑回归）：分类基础，Sigmoid 函数，评估指标
- ⭐⭐⭐ 第 5 章（决策树）：信息增益，基尼不纯度，决策树构建
- ⭐⭐⭐ 第 6 章（随机森林）：集成学习，Bootstrap，特征重要性
- ⭐⭐⭐ 第 7 章（KNN）：距离度量，K 值选择，特征缩放
- ⭐⭐ 第 3 章（线性回归）：最小二乘法，正则化，模型评估
- ⭐⭐ 第 8 章（SVM）：支持向量，核函数，间隔最大化

---

**祝你考试顺利！Good luck! 🍀**
