# Chapter 6: Clustering and the k-Means Algorithm

## üìö Concepts

### Core Ideas of Clustering

- **Clustering**: Grouping data objects so that points in the same group are similar, while points in different groups are dissimilar.
- **Unsupervised Learning**: Learning patterns without labeled data; algorithms discover structure automatically.
- **Cluster**: A collection of similar data points.
- **Centroid**: The center of a cluster, typically the mean of the points assigned to the cluster.
- **Inertia**: Sum of squared distances from each sample to its nearest centroid; measures cluster compactness.

### Types of Clustering

1. **Partitioning Clustering**
   - Splits data into k non-overlapping clusters.
   - Examples: k-Means, k-Medoids.

2. **Hierarchical Clustering**
   - Builds a tree of clusters (dendrogram).
   - Examples: agglomerative and divisive clustering.

3. **Density-Based Clustering**
   - Finds clusters of arbitrary shape based on density.
   - Examples: DBSCAN, OPTICS.

4. **Grid-Based Clustering**
   - Divides the data space into grid cells.
   - Examples: STING, CLIQUE.

5. **Model-Based Clustering**
   - Assumes data is generated by a mixture of probabilistic models.
   - Example: Gaussian Mixture Models (GMM).

### k-Means Essentials

- **Objective**: Minimize within-cluster sum of squares (WCSS).
  $$J = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2$$

- **Category**: Partitioning clustering, iterative optimization.
- **Time Complexity**: O(n √ó k √ó i √ó d)
  - n: number of samples
  - k: number of clusters
  - i: iterations
  - d: feature dimensions

### k-Means Variants

- **k-Means++**: Improved centroid initialization.
- **Mini-Batch k-Means**: Uses small batches for faster updates.
- **k-Medoids (PAM)**: Uses actual data points as centers.
- **Fuzzy k-Means**: Soft clustering; points can partially belong to multiple clusters.

## üîç Explanation

### k-Means Algorithm Steps

```
1. Initialize: randomly select k centroids.
2. Repeat until convergence:
   a. Assignment step: assign each point to the nearest centroid.
   b. Update step: recompute each centroid as the mean of its assigned points.
3. Output: cluster assignments and centroid positions.
```

**Assignment Step:**
$$C_i^{(t)} = \{x_p : ||x_p - \mu_i^{(t)}||^2 \leq ||x_p - \mu_j^{(t)}||^2, \forall j\}$$

**Update Step:**
$$\mu_i^{(t+1)} = \frac{1}{|C_i^{(t)}|} \sum_{x_j \in C_i^{(t)}} x_j$$

**Convergence Criteria:**

- Centroids stop changing significantly.
- Cluster assignments remain unchanged.
- Objective drops below a threshold.
- Maximum iterations reached.

### k-Means in Action

```python
import numpy as np
import matplotlib.pyplot as plt

# Generate data
np.random.seed(42)
X1 = np.random.randn(100, 2) + [2, 2]
X2 = np.random.randn(100, 2) + [-2, -2]
X3 = np.random.randn(100, 2) + [2, -2]
X = np.vstack([X1, X2, X3])

k = 3
max_iters = 10
centroids = X[np.random.choice(X.shape[0], k, replace=False)]

for iteration in range(max_iters):
    distances = np.sqrt(((centroids[:, np.newaxis] - X)**2).sum(axis=2))
    labels = np.argmin(distances, axis=0)

    new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])
    if np.allclose(centroids, new_centroids):
        print(f'Converged at iteration {iteration + 1}')
        break
    centroids = new_centroids

plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, edgecolors='black')
plt.title('k-Means Clustering Result')
plt.show()
```

### k-Means++ Initialization

**Problem with Random Initialization:**

- May converge to local optima.
- Requires more iterations.
- Unstable results across runs.

**k-Means++ Solution:**

```
1. Pick the first centroid at random.
2. For each subsequent centroid:
   a. Compute distance squared from each point to its nearest chosen centroid.
   b. Sample the next centroid with probability proportional to the squared distance.
3. Repeat until k centroids are chosen.
```

**Advantages:**

- Better starting positions.
- Faster convergence.
- More stable solutions.
- Provides an O(log k) approximation guarantee.

```python
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, random_state=42)
kmeans.fit(X)
```

### Choosing k

**1. Elbow Method**

```python
wcss = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss, 'bo-')
plt.xlabel('k')
plt.ylabel('Within-cluster Sum of Squares')
plt.title('Elbow Method')
plt.show()
```

Look for the ‚Äúelbow‚Äù where the rate of decrease sharply changes.

**2. Silhouette Score**

$$s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}$$

- a(i): average distance to points in the same cluster.
- b(i): minimum average distance to points in other clusters.
- Range: [-1, 1]; higher is better.

```python
from sklearn.metrics import silhouette_score

scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X)
    scores.append(silhouette_score(X, labels))

plt.plot(range(2, 11), scores, 'ro-')
plt.xlabel('k')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Analysis')
plt.show()
```

### Mini-Batch k-Means

- Processes small batches to reduce computation and memory usage.
- Converges faster on large datasets.
- Trade-off: Slightly less accurate than standard k-Means.

```python
from sklearn.cluster import MiniBatchKMeans

mbk = MiniBatchKMeans(n_clusters=3, batch_size=64, random_state=42)
labels = mbk.fit_predict(X)
```

### Limitations of k-Means

- Sensitive to initialization and outliers.
- Assumes spherical clusters of similar size.
- Requires specifying k in advance.
- Uses Euclidean distance (limited for categorical features).

**Remedies:**

- Use k-Means++ initialization.
- Scale features before clustering.
- Apply outlier detection beforehand.
- Try alternative algorithms (GMM, DBSCAN) when assumptions do not hold.

## üìú History

- **1957**: Lloyd proposed the original algorithm (published in 1982).
- **1967**: MacQueen formalized k-Means.
- **1990s**: k-Means++ and k-Medoids variants emerged.
- **2000s**: Mini-Batch k-Means introduced for large-scale data.
- **2010s to Present**: Widespread industrial adoption, GPU acceleration, and integration with streaming data platforms.

## üí™ Exercises

1. Implement k-Means from scratch and compare results to scikit-learn‚Äôs implementation.
2. Use the elbow method and silhouette score to determine k for the Iris dataset.
3. Apply Mini-Batch k-Means to a 1-million-point dataset and compare runtime with standard k-Means.
4. Experiment with k-Means on non-spherical clusters (e.g., moons dataset) and analyze performance.
5. Combine PCA with k-Means to cluster high-dimensional data; evaluate with silhouette scores.

## üß™ Projects

**Project 1: Customer Segmentation**

- Dataset: Retail customer purchasing behavior.
- Tasks: Feature engineering, scale features, choose k using elbow/silhouette, interpret clusters for marketing strategies.

**Project 2: Image Compression**

- Use k-Means to reduce the number of colors in an image.
- Evaluate quality vs. compression ratio.
- Visualize the effect of different k values.

**Project 3: Document Clustering**

- Convert text to TF-IDF vectors.
- Use k-Means and analyze top terms per cluster.
- Visualize clusters using t-SNE or UMAP.

## üéØ Check Your Understanding

### Multiple Choice

1. Which clustering technique assumes spherical clusters of similar size?
   - A. DBSCAN
   - B. Hierarchical clustering
   - C. k-Means
   - D. Gaussian Mixture Models

2. What does k-Means optimize?
   - A. Between-cluster variance
   - B. Within-cluster sum of squares
   - C. Cluster silhouette scores
   - D. Density estimates

3. The main advantage of k-Means++ is:
   - A. Fewer required features
   - B. Better initialization leading to faster convergence
   - C. Ability to handle categorical data
   - D. Avoiding the need to specify k

4. Mini-Batch k-Means trades accuracy for:
   - A. Higher interpretability
   - B. Faster computation
   - C. Better handling of outliers
   - D. More stable initialization

5. A negative silhouette score indicates:
   - A. Perfect clustering
   - B. Point lies on the cluster boundary
   - C. Potential misclassification
   - D. Clusters are too compact

### True or False

1. k-Means can detect clusters of arbitrary shape. (False)
2. Scaling features is optional for k-Means. (False)
3. k-Means++ selects initial centroids probabilistically. (True)
4. Mini-Batch k-Means always produces identical results to standard k-Means. (False)
5. k is the only hyperparameter in k-Means. (False)
6. GMM can be seen as a soft version of k-Means. (True)
7. k-Means is robust to outliers. (False)
8. The elbow method provides a definitive k value. (False)

### Short Answer

1. Explain why k-Means struggles with clusters of different densities.
2. How would you modify k-Means to handle categorical variables?
3. Describe the steps to integrate PCA before applying k-Means.
4. When would DBSCAN be preferred over k-Means?
5. How can you detect if k-Means has converged to a local minimum?

## üß† Advanced Topics

- Soft clustering with Gaussian Mixture Models.
- Clustering evaluation without labels (Calinski-Harabasz, Davies-Bouldin indices).
- Clustering streaming data with incremental k-Means.
- Combining clustering with anomaly detection.
- Accelerated k-Means using KD-trees and GPU computation.
- Semi-supervised clustering with constraints (must-link / cannot-link).

## üîß Tools and Libraries

- **scikit-learn**: `KMeans`, `MiniBatchKMeans`, `GaussianMixture`, `AgglomerativeClustering`.
- **scipy**: Hierarchical clustering implementation.
- **cuML**: GPU-accelerated k-Means.
- **Yellowbrick**: Visual diagnostic tools for clustering (elbow, silhouette).
- **UMAP / t-SNE**: Dimensionality reduction for visualization.

## üìñ Learning Resources

- *Pattern Recognition and Machine Learning* ‚Äì Christopher Bishop.
- scikit-learn clustering guide and examples.
- Coursera: Clustering and Retrieval (University of Washington).
- Kaggle micro-courses on unsupervised learning.
- Andrew Ng‚Äôs Unsupervised Learning lectures.

## üéØ Learning Objectives Checklist

After completing this chapter, you should be able to:

- ‚úÖ Describe the main families of clustering algorithms.
- ‚úÖ Implement and explain the k-Means algorithm.
- ‚úÖ Choose an appropriate number of clusters using elbow and silhouette analyses.
- ‚úÖ Apply k-Means++, Mini-Batch k-Means, and understand their trade-offs.
- ‚úÖ Recognize the limitations of k-Means and select alternative clustering methods when needed.
- ‚úÖ Combine clustering with dimensionality reduction and evaluation metrics.

---

**Course wrap-up:** Consolidate clustering knowledge with advanced techniques and prepare to integrate clustering into machine learning pipelines.
