# ç¬¬å…­ç« ï¼šèšç±»ä¸ k-Means ç®—æ³•

## ğŸ“š æ¦‚å¿µ

### èšç±»æ ¸å¿ƒæ¦‚å¿µ

- **èšç±»ï¼ˆClusteringï¼‰**ï¼šå°†æ•°æ®å¯¹è±¡åˆ†ç»„ï¼Œä½¿åŒç»„å¯¹è±¡ç›¸ä¼¼åº¦é«˜ï¼Œä¸åŒç»„å¯¹è±¡ç›¸ä¼¼åº¦ä½
- **æ— ç›‘ç£å­¦ä¹ **ï¼šæ²¡æœ‰æ ‡è®°æ•°æ®ï¼Œç®—æ³•è‡ªåŠ¨å‘ç°æ•°æ®ç»“æ„
- **ç°‡ï¼ˆClusterï¼‰**ï¼šä¸€ç»„ç›¸ä¼¼çš„æ•°æ®ç‚¹é›†åˆ
- **è´¨å¿ƒï¼ˆCentroidï¼‰**ï¼šç°‡çš„ä¸­å¿ƒç‚¹ï¼Œé€šå¸¸æ˜¯ç°‡å†…æ‰€æœ‰ç‚¹çš„å¹³å‡å€¼
- **æƒ¯æ€§ï¼ˆInertiaï¼‰**ï¼šæ ·æœ¬åˆ°æœ€è¿‘è´¨å¿ƒçš„è·ç¦»å¹³æ–¹å’Œï¼Œè¡¡é‡ç°‡çš„ç´§å¯†åº¦

### èšç±»ç±»å‹

1. **åˆ’åˆ†èšç±»ï¼ˆPartitioning Clusteringï¼‰**

   - å°†æ•°æ®åˆ†æˆ k ä¸ªä¸é‡å çš„ç°‡
   - ç¤ºä¾‹ï¼šk-Means, k-Medoids

2. **å±‚æ¬¡èšç±»ï¼ˆHierarchical Clusteringï¼‰**

   - æ„å»ºç°‡çš„æ ‘çŠ¶ç»“æ„
   - ç¤ºä¾‹ï¼šå‡èšå±‚æ¬¡èšç±»ã€åˆ†è£‚å±‚æ¬¡èšç±»

3. **åŸºäºå¯†åº¦çš„èšç±»ï¼ˆDensity-Based Clusteringï¼‰**

   - åŸºäºæ•°æ®å¯†åº¦å‘ç°ä»»æ„å½¢çŠ¶çš„ç°‡
   - ç¤ºä¾‹ï¼šDBSCAN, OPTICS

4. **åŸºäºç½‘æ ¼çš„èšç±»ï¼ˆGrid-Based Clusteringï¼‰**

   - å°†ç©ºé—´åˆ’åˆ†ä¸ºç½‘æ ¼å•å…ƒ
   - ç¤ºä¾‹ï¼šSTING, CLIQUE

5. **åŸºäºæ¨¡å‹çš„èšç±»ï¼ˆModel-Based Clusteringï¼‰**
   - å‡è®¾æ•°æ®ç”±å¤šä¸ªæ¦‚ç‡åˆ†å¸ƒç”Ÿæˆ
   - ç¤ºä¾‹ï¼šé«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰

### k-Means ç®—æ³•æ ¸å¿ƒ

- **ç›®æ ‡**ï¼šæœ€å°åŒ–ç°‡å†…å¹³æ–¹å’Œï¼ˆWCSSï¼‰
  $$J = \sum_{i=1}^{k} \sum_{x \in C_i} ||x - \mu_i||^2$$

- **ç®—æ³•ç±»å‹**ï¼šåˆ’åˆ†èšç±»ã€è¿­ä»£ä¼˜åŒ–
- **æ—¶é—´å¤æ‚åº¦**ï¼šO(n Ã— k Ã— i Ã— d)
  - nï¼šæ ·æœ¬æ•°
  - kï¼šç°‡æ•°
  - iï¼šè¿­ä»£æ¬¡æ•°
  - dï¼šç‰¹å¾ç»´åº¦

### k-Means å˜ä½“

- **k-Means++**ï¼šæ”¹è¿›çš„è´¨å¿ƒåˆå§‹åŒ–
- **Mini-Batch k-Means**ï¼šä½¿ç”¨å°æ‰¹é‡æ•°æ®ï¼Œæ›´å¿«
- **k-Medoids (PAM)**ï¼šä½¿ç”¨å®é™…æ•°æ®ç‚¹ä½œä¸ºä¸­å¿ƒ
- **Fuzzy k-Means**ï¼šè½¯èšç±»ï¼Œç‚¹å¯ä»¥éƒ¨åˆ†å±äºå¤šä¸ªç°‡

## ğŸ” è§£é‡Š

### k-Means ç®—æ³•è¯¦è§£

**ç®—æ³•æ­¥éª¤ï¼š**

```
1. åˆå§‹åŒ–ï¼šéšæœºé€‰æ‹©kä¸ªè´¨å¿ƒ
2. é‡å¤ç›´åˆ°æ”¶æ•›ï¼š
   a. åˆ†é…æ­¥éª¤ï¼šå°†æ¯ä¸ªç‚¹åˆ†é…ç»™æœ€è¿‘çš„è´¨å¿ƒ
   b. æ›´æ–°æ­¥éª¤ï¼šé‡æ–°è®¡ç®—æ¯ä¸ªç°‡çš„è´¨å¿ƒ
3. è¾“å‡ºï¼šæœ€ç»ˆçš„ç°‡åˆ†é…å’Œè´¨å¿ƒä½ç½®
```

**æ•°å­¦è¡¨è¾¾ï¼š**

**åˆ†é…æ­¥éª¤ï¼š**
$$C_i^{(t)} = \{x_p : ||x_p - \mu_i^{(t)}||^2 \leq ||x_p - \mu_j^{(t)}||^2, \forall j\}$$

**æ›´æ–°æ­¥éª¤ï¼š**
$$\mu_i^{(t+1)} = \frac{1}{|C_i^{(t)}|} \sum_{x_j \in C_i^{(t)}} x_j$$

**æ”¶æ•›æ¡ä»¶ï¼š**

- è´¨å¿ƒä¸å†å˜åŒ–
- ç°‡åˆ†é…ä¸å†å˜åŒ–
- ç›®æ ‡å‡½æ•°å˜åŒ–å°äºé˜ˆå€¼
- è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°

### k-Means çš„å·¥ä½œç¤ºä¾‹

```python
import numpy as np
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•°æ®
np.random.seed(42)
X1 = np.random.randn(100, 2) + [2, 2]
X2 = np.random.randn(100, 2) + [-2, -2]
X3 = np.random.randn(100, 2) + [2, -2]
X = np.vstack([X1, X2, X3])

# k-Meansæ­¥éª¤æ¼”ç¤º
k = 3
max_iters = 10

# 1. éšæœºåˆå§‹åŒ–è´¨å¿ƒ
centroids = X[np.random.choice(X.shape[0], k, replace=False)]

for iteration in range(max_iters):
    # 2. åˆ†é…æ­¥éª¤
    distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))
    labels = np.argmin(distances, axis=0)

    # 3. æ›´æ–°æ­¥éª¤
    new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])

    # æ£€æŸ¥æ”¶æ•›
    if np.allclose(centroids, new_centroids):
        print(f'åœ¨ç¬¬{iteration+1}æ¬¡è¿­ä»£æ”¶æ•›')
        break

    centroids = new_centroids

# å¯è§†åŒ–ç»“æœ
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, edgecolors='black')
plt.title('k-Meansèšç±»ç»“æœ')
plt.show()
```

### k-Means++åˆå§‹åŒ–

**é—®é¢˜**ï¼šéšæœºåˆå§‹åŒ–å¯èƒ½å¯¼è‡´ï¼š

- æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜
- éœ€è¦æ›´å¤šè¿­ä»£
- ä¸ç¨³å®šçš„ç»“æœ

**k-Means++è§£å†³æ–¹æ¡ˆï¼š**

```
1. éšæœºé€‰æ‹©ç¬¬ä¸€ä¸ªè´¨å¿ƒ
2. å¯¹äºæ¯ä¸ªåç»­è´¨å¿ƒï¼š
   a. è®¡ç®—æ¯ä¸ªç‚¹åˆ°æœ€è¿‘å·²é€‰è´¨å¿ƒçš„è·ç¦»
   b. ä»¥è·ç¦»å¹³æ–¹æˆæ­£æ¯”çš„æ¦‚ç‡é€‰æ‹©ä¸‹ä¸€ä¸ªè´¨å¿ƒ
3. è¿™æ ·è´¨å¿ƒä¼šæ›´åˆ†æ•£
```

**ä¼˜ç‚¹ï¼š**

- æ›´å¥½çš„åˆå§‹åŒ–
- æ›´å¿«æ”¶æ•›
- æ›´ç¨³å®šçš„ç»“æœ
- O(log k)ç«äº‰æ¯”ä¿è¯

```python
from sklearn.cluster import KMeans

# ä½¿ç”¨k-Means++åˆå§‹åŒ–
kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, random_state=42)
kmeans.fit(X)
```

### é€‰æ‹©æœ€ä¼˜ k å€¼

**1. è‚˜éƒ¨æ³•ï¼ˆElbow Methodï¼‰**

```python
wcss = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

plt.plot(K_range, wcss, 'bo-')
plt.xlabel('kå€¼')
plt.ylabel('WCSSï¼ˆç°‡å†…å¹³æ–¹å’Œï¼‰')
plt.title('è‚˜éƒ¨æ³•ç¡®å®šæœ€ä¼˜k')
plt.show()
```

å¯»æ‰¾"è‚˜éƒ¨"ï¼šWCSS ä¸‹é™é€Ÿç‡æ€¥å‰§å˜åŒ–çš„ç‚¹

**2. è½®å»“ç³»æ•°ï¼ˆSilhouette Scoreï¼‰**
$$s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}}$$

å…¶ä¸­ï¼š

- a(i)ï¼šç‚¹ i åˆ°åŒç°‡å…¶ä»–ç‚¹çš„å¹³å‡è·ç¦»
- b(i)ï¼šç‚¹ i åˆ°æœ€è¿‘å…¶ä»–ç°‡ç‚¹çš„å¹³å‡è·ç¦»
- èŒƒå›´ï¼š[-1, 1]
  - æ¥è¿‘ 1ï¼šèšç±»è‰¯å¥½
  - æ¥è¿‘ 0ï¼šåœ¨ç°‡è¾¹ç•Œ
  - è´Ÿå€¼ï¼šå¯èƒ½åˆ†é…é”™è¯¯

```python
from sklearn.metrics import silhouette_score

silhouette_scores = []
K_range = range(2, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    silhouette_scores.append(score)

plt.plot(K_range, silhouette_scores, 'bo-')
plt.xlabel('kå€¼')
plt.ylabel('è½®å»“ç³»æ•°')
plt.title('è½®å»“ç³»æ•°æ³•ç¡®å®šæœ€ä¼˜k')
plt.show()

# é€‰æ‹©æœ€å¤§è½®å»“ç³»æ•°å¯¹åº”çš„k
optimal_k = K_range[np.argmax(silhouette_scores)]
```

**3. é—´éš™ç»Ÿè®¡ï¼ˆGap Statisticï¼‰**
æ¯”è¾ƒ WCSS ä¸éšæœºæ•°æ®çš„æœŸæœ› WCSSï¼š
$$Gap(k) = E[\log(W_k)] - \log(W_k)$$

é€‰æ‹©ä½¿ Gap(k)æœ€å¤§çš„ kã€‚

**4. Davies-Bouldin æŒ‡æ•°**
$$DB = \frac{1}{k} \sum_{i=1}^{k} \max_{j \neq i} \frac{\sigma_i + \sigma_j}{d(c_i, c_j)}$$

- å€¼è¶Šå°è¶Šå¥½
- è¡¡é‡ç°‡å†…ç›¸ä¼¼åº¦ä¸ç°‡é—´å·®å¼‚åº¦çš„æ¯”ç‡

**5. Calinski-Harabasz æŒ‡æ•°**
$$CH = \frac{SS_B / (k-1)}{SS_W / (n-k)}$$

- SS_Bï¼šç°‡é—´å¹³æ–¹å’Œ
- SS_Wï¼šç°‡å†…å¹³æ–¹å’Œ
- å€¼è¶Šå¤§è¶Šå¥½

```python
from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score

for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X)

    db_score = davies_bouldin_score(X, labels)
    ch_score = calinski_harabasz_score(X, labels)

    print(f'k={k}: DB={db_score:.3f}, CH={ch_score:.3f}')
```

### k-Means çš„ä¼˜åŠ¿ä¸å±€é™

**ä¼˜åŠ¿ï¼š**

1. âœ… **ç®€å•ç›´è§‚**ï¼šæ˜“äºç†è§£å’Œå®ç°
2. âœ… **é«˜æ•ˆ**ï¼šè®¡ç®—å¤æ‚åº¦ç›¸å¯¹è¾ƒä½
3. âœ… **å¯æ‰©å±•**ï¼šé€‚ç”¨äºå¤§æ•°æ®é›†
4. âœ… **ä¿è¯æ”¶æ•›**ï¼šæ€»æ˜¯æ”¶æ•›ï¼ˆå¯èƒ½æ˜¯å±€éƒ¨æœ€ä¼˜ï¼‰
5. âœ… **é€‚åˆçƒå½¢ç°‡**ï¼šç°‡å¤§å°ç›¸ä¼¼ã€å¯†åº¦å‡åŒ€æ—¶æ•ˆæœå¥½

**å±€é™æ€§ï¼š**

1. âŒ **éœ€è¦é¢„å…ˆæŒ‡å®š k**ï¼šä¸çŸ¥é“çœŸå®ç°‡æ•°
2. âŒ **å¯¹åˆå§‹åŒ–æ•æ„Ÿ**ï¼šä¸åŒåˆå§‹åŒ–å¯èƒ½å¾—åˆ°ä¸åŒç»“æœ
3. âŒ **å‡è®¾çƒå½¢ç°‡**ï¼šä¸é€‚åˆä»»æ„å½¢çŠ¶çš„ç°‡
4. âŒ **å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ**ï¼šå¼‚å¸¸å€¼ä¼šæ‰­æ›²è´¨å¿ƒ
5. âŒ **å‡è®¾ç°‡å¤§å°ç›¸ä¼¼**ï¼šå¤§å°å·®å¼‚å¤§æ—¶è¡¨ç°å·®
6. âŒ **åªé€‚ç”¨äºæ•°å€¼æ•°æ®**ï¼šéœ€è¦è®¡ç®—è·ç¦»

**è§£å†³æ–¹æ¡ˆï¼š**

- k-Means++æ”¹è¿›åˆå§‹åŒ–
- å¤šæ¬¡è¿è¡Œé€‰æ‹©æœ€ä½³ç»“æœ
- é¢„å¤„ç†å»é™¤å¼‚å¸¸å€¼
- æ ‡å‡†åŒ–ç‰¹å¾
- ä½¿ç”¨å…¶ä»–èšç±»ç®—æ³•ï¼ˆDBSCANã€GMMï¼‰

### Mini-Batch k-Means

**åŠ¨æœº**ï¼šå¤§æ•°æ®é›†ä¸Š k-Means å¾ˆæ…¢

**æ–¹æ³•**ï¼š

- æ¯æ¬¡è¿­ä»£ä½¿ç”¨å°æ‰¹é‡æ•°æ®
- æ›´æ–°è´¨å¿ƒæ—¶ä½¿ç”¨ç§»åŠ¨å¹³å‡
- æ›´å¿«ä½†è´¨é‡ç•¥æœ‰ä¸‹é™

```python
from sklearn.cluster import MiniBatchKMeans

# Mini-Batch k-Means
mb_kmeans = MiniBatchKMeans(
    n_clusters=3,
    batch_size=100,
    max_iter=100,
    random_state=42
)
mb_kmeans.fit(X)

# æ¯”è¾ƒæ—¶é—´
import time

# æ ‡å‡†k-Means
start = time.time()
kmeans = KMeans(n_clusters=3).fit(X)
time_kmeans = time.time() - start

# Mini-Batch k-Means
start = time.time()
mb_kmeans = MiniBatchKMeans(n_clusters=3).fit(X)
time_mb = time.time() - start

print(f'æ ‡å‡†k-Means: {time_kmeans:.4f}ç§’')
print(f'Mini-Batch k-Means: {time_mb:.4f}ç§’')
```

### k-Medoids (PAM)

**ä¸ k-Means çš„åŒºåˆ«ï¼š**

- ä½¿ç”¨å®é™…æ•°æ®ç‚¹ä½œä¸ºä¸­å¿ƒï¼ˆmedoidï¼‰ï¼Œè€Œéå‡å€¼
- å¯¹å¼‚å¸¸å€¼æ›´ç¨³å¥
- å¯ç”¨äºéæ¬§å‡ é‡Œå¾—è·ç¦»
- è®¡ç®—æˆæœ¬æ›´é«˜ï¼šO(k(n-k)Â²)

```python
from sklearn_extra.cluster import KMedoids

kmedoids = KMedoids(n_clusters=3, random_state=42)
kmedoids.fit(X)
```

## ğŸ“œ å†å²

### èšç±»çš„å‘å±•å†ç¨‹

**1950-1960 å¹´ä»£ - èµ·æº**

- 1957 å¹´ï¼š**èšç±»åˆ†ææ¦‚å¿µ**é¦–æ¬¡åœ¨å¿ƒç†å­¦ä¸­ä½¿ç”¨
- 1965 å¹´ï¼š**å•é“¾æ¥å’Œå®Œå…¨é“¾æ¥**å±‚æ¬¡èšç±»æ–¹æ³•

**1960-1970 å¹´ä»£ - å¥ åŸºæ—¶æœŸ**

- **1967 å¹´ï¼šk-Means ç®—æ³•**
  - MacQueen é¦–æ¬¡æå‡ºæœ¯è¯­"k-means"
  - ä½†æ€æƒ³æ›´æ—©ç”± Stuart Lloyd (1957)å’Œ Edward Forgy (1965)ç‹¬ç«‹æå‡º
- å±‚æ¬¡èšç±»æ–¹æ³•å‘å±•

**1980 å¹´ä»£ - ç†è®ºå‘å±•**

- **1987 å¹´ï¼šPAM ç®—æ³•**ï¼ˆk-Medoidsï¼‰
  - Kaufman & Rousseeuw æå‡º
- æ¨¡ç³Šèšç±»ç†è®ºå‘å±•
- DBSCAN ç®—æ³•çš„ç†è®ºåŸºç¡€

**1990 å¹´ä»£ - å¯†åº¦å’Œç½‘æ ¼æ–¹æ³•**

- **1996 å¹´ï¼šDBSCAN**
  - Ester, Kriegel, Sander, Xu æå‡º
  - é©å‘½æ€§çš„åŸºäºå¯†åº¦çš„èšç±»
- BIRCH (1996)ï¼šå¤§æ•°æ®é›†å±‚æ¬¡èšç±»
- CURE (1998)ï¼šå¤„ç†ä»»æ„å½¢çŠ¶ç°‡

**2000 å¹´ä»£ - å¯æ‰©å±•æ€§å’Œæ”¹è¿›**

- **2007 å¹´ï¼šk-Means++**
  - Arthur & Vassilvitskii æå‡º
  - æ”¹è¿›åˆå§‹åŒ–ï¼Œç†è®ºä¿è¯
- Spectral Clustering æµè¡Œ
- å¤§è§„æ¨¡æ•°æ®èšç±»ç®—æ³•

**2010 å¹´ä»£è‡³ä»Š - æ·±åº¦å­¦ä¹ æ—¶ä»£**

- æ·±åº¦èšç±»æ–¹æ³•
- åœ¨çº¿/æµå¼èšç±»
- GPU åŠ é€Ÿèšç±»
- AutoML è‡ªåŠ¨ç¡®å®šç°‡æ•°
- ä¸æ·±åº¦å­¦ä¹ ç»“åˆï¼ˆDeep Embedded Clusteringï¼‰

### å…³é”®è´¡çŒ®è€…

- **Stuart Lloyd**ï¼šk-Means ç®—æ³•ï¼ˆ1957ï¼Œ1982 å¹´å‘è¡¨ï¼‰
- **J. MacQueen**ï¼šåˆ›é€ "k-means"æœ¯è¯­ï¼ˆ1967ï¼‰
- **David Arthur & Sergei Vassilvitskii**ï¼šk-Means++ç®—æ³•
- **Martin Ester ç­‰**ï¼šDBSCAN ç®—æ³•
- **Kaufman & Rousseeuw**ï¼šk-Medoids ç®—æ³•

### é‡Œç¨‹ç¢‘è®ºæ–‡

- Lloyd, S. (1982). "Least Squares Quantization in PCM"
- Arthur, D. & Vassilvitskii, S. (2007). "k-means++: The Advantages of Careful Seeding"
- Ester, M. et al. (1996). "A Density-Based Algorithm for Discovering Clusters"

## ğŸ’ª ç»ƒä¹ 

### åŸºç¡€ç»ƒä¹ 

**ç»ƒä¹  1ï¼šæ‰‹å·¥ k-Means**
ç»™å®šæ•°æ®ç‚¹ï¼šA(1,1), B(2,1), C(4,3), D(5,4)
k=2ï¼Œåˆå§‹è´¨å¿ƒï¼šÎ¼â‚=(1,1), Î¼â‚‚=(5,4)

å®Œæˆ 3 æ¬¡è¿­ä»£ï¼š

1. åˆ†é…æ¯ä¸ªç‚¹åˆ°æœ€è¿‘è´¨å¿ƒ
2. è®¡ç®—æ–°è´¨å¿ƒ
3. é‡å¤

**ç»ƒä¹  2ï¼šè®¡ç®— WCSS**
ç°‡ 1ï¼š[(1,1), (2,2), (2,1)]ï¼Œè´¨å¿ƒ(1.67, 1.33)
ç°‡ 2ï¼š[(5,5), (6,6), (5,6)]ï¼Œè´¨å¿ƒ(5.33, 5.67)

è®¡ç®—æ€» WCSSã€‚

**ç»ƒä¹  3ï¼šè½®å»“ç³»æ•°è®¡ç®—**
å¯¹äºç‚¹ xâ‚ï¼Œå®ƒåˆ°åŒç°‡å…¶ä»–ç‚¹çš„å¹³å‡è·ç¦» a=2ï¼Œåˆ°æœ€è¿‘å…¶ä»–ç°‡çš„å¹³å‡è·ç¦» b=5ã€‚
è®¡ç®— xâ‚ çš„è½®å»“ç³»æ•°ã€‚

### å®è·µé¡¹ç›®

**é¡¹ç›® 1ï¼šå®Œæ•´çš„ k-Means å®ç°**

```python
import numpy as np
import matplotlib.pyplot as plt

class KMeansCustom:
    def __init__(self, n_clusters=3, max_iters=100, tol=1e-4, random_state=None):
        self.n_clusters = n_clusters
        self.max_iters = max_iters
        self.tol = tol
        self.random_state = random_state
        self.centroids = None
        self.labels = None
        self.inertia_ = None

    def initialize_centroids(self, X):
        """éšæœºåˆå§‹åŒ–è´¨å¿ƒ"""
        np.random.seed(self.random_state)
        indices = np.random.choice(X.shape[0], self.n_clusters, replace=False)
        return X[indices]

    def assign_clusters(self, X, centroids):
        """åˆ†é…æ¯ä¸ªç‚¹åˆ°æœ€è¿‘çš„è´¨å¿ƒ"""
        distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))
        return np.argmin(distances, axis=0)

    def update_centroids(self, X, labels):
        """æ›´æ–°è´¨å¿ƒä¸ºç°‡å†…ç‚¹çš„å‡å€¼"""
        centroids = np.zeros((self.n_clusters, X.shape[1]))
        for k in range(self.n_clusters):
            if np.sum(labels == k) > 0:
                centroids[k] = X[labels == k].mean(axis=0)
        return centroids

    def compute_inertia(self, X, labels, centroids):
        """è®¡ç®—ç°‡å†…å¹³æ–¹å’Œ"""
        inertia = 0
        for k in range(self.n_clusters):
            cluster_points = X[labels == k]
            if len(cluster_points) > 0:
                inertia += ((cluster_points - centroids[k])**2).sum()
        return inertia

    def fit(self, X):
        """è®­ç»ƒk-Meansæ¨¡å‹"""
        # åˆå§‹åŒ–
        self.centroids = self.initialize_centroids(X)

        for iteration in range(self.max_iters):
            # åˆ†é…ç°‡
            labels = self.assign_clusters(X, self.centroids)

            # æ›´æ–°è´¨å¿ƒ
            new_centroids = self.update_centroids(X, labels)

            # æ£€æŸ¥æ”¶æ•›
            if np.allclose(self.centroids, new_centroids, atol=self.tol):
                print(f'åœ¨ç¬¬{iteration+1}æ¬¡è¿­ä»£æ”¶æ•›')
                break

            self.centroids = new_centroids

        self.labels = labels
        self.inertia_ = self.compute_inertia(X, labels, self.centroids)

        return self

    def predict(self, X):
        """é¢„æµ‹æ–°æ•°æ®çš„ç°‡æ ‡ç­¾"""
        return self.assign_clusters(X, self.centroids)

    def fit_predict(self, X):
        """è®­ç»ƒå¹¶è¿”å›æ ‡ç­¾"""
        self.fit(X)
        return self.labels

# æµ‹è¯•
np.random.seed(42)
X1 = np.random.randn(100, 2) + [2, 2]
X2 = np.random.randn(100, 2) + [-2, -2]
X3 = np.random.randn(100, 2) + [2, -2]
X = np.vstack([X1, X2, X3])

kmeans = KMeansCustom(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

# å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6)
plt.scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1],
           c='red', marker='X', s=200, edgecolors='black', linewidths=2)
plt.title(f'è‡ªå®šä¹‰k-Meansèšç±» (Inertia={kmeans.inertia_:.2f})')
plt.xlabel('ç‰¹å¾1')
plt.ylabel('ç‰¹å¾2')
plt.colorbar(label='ç°‡æ ‡ç­¾')
plt.show()
```

**é¡¹ç›® 2ï¼šå®¢æˆ·ç»†åˆ†åˆ†æ**

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# ç”Ÿæˆå®¢æˆ·æ•°æ®
np.random.seed(42)
n_customers = 500

data = {
    'å¹´é¾„': np.random.randint(18, 70, n_customers),
    'å¹´æ”¶å…¥': np.random.randint(15000, 150000, n_customers),
    'æ¶ˆè´¹è¯„åˆ†': np.random.randint(1, 100, n_customers),
    'è´­ä¹°é¢‘ç‡': np.random.randint(1, 50, n_customers)
}
df = pd.DataFrame(data)

# æ ‡å‡†åŒ–
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

# è‚˜éƒ¨æ³•ç¡®å®šæœ€ä¼˜k
wcss = []
K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(10, 6))
plt.plot(K_range, wcss, 'bo-')
plt.xlabel('ç°‡æ•° k')
plt.ylabel('WCSS')
plt.title('è‚˜éƒ¨æ³•ç¡®å®šæœ€ä¼˜k')
plt.grid(True)
plt.show()

# ä½¿ç”¨k=4è¿›è¡Œèšç±»
optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df['ç°‡'] = kmeans.fit_predict(X_scaled)

# åˆ†ææ¯ä¸ªç°‡çš„ç‰¹å¾
print('\nå„ç°‡ç‰¹å¾ç»Ÿè®¡ï¼š')
print(df.groupby('ç°‡').mean())

# PCAé™ç»´å¯è§†åŒ–
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(12, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['ç°‡'], cmap='viridis', alpha=0.6)
plt.xlabel(f'ä¸»æˆåˆ†1 ({pca.explained_variance_ratio_[0]:.2%}æ–¹å·®)')
plt.ylabel(f'ä¸»æˆåˆ†2 ({pca.explained_variance_ratio_[1]:.2%}æ–¹å·®)')
plt.title('å®¢æˆ·ç»†åˆ†ï¼ˆPCAé™ç»´å¯è§†åŒ–ï¼‰')
plt.colorbar(scatter, label='ç°‡æ ‡ç­¾')
plt.show()

# ä¸ºæ¯ä¸ªç°‡å‘½å
cluster_names = {
    0: 'é«˜æ”¶å…¥é«˜æ¶ˆè´¹',
    1: 'å¹´è½»ä½æ”¶å…¥',
    2: 'ä¸­å¹´ä¸­ç­‰æ”¶å…¥',
    3: 'ä½æ¶ˆè´¹ç¨³å®š'
}

for cluster_id, name in cluster_names.items():
    cluster_data = df[df['ç°‡'] == cluster_id]
    print(f'\n{name}ï¼ˆç°‡{cluster_id}ï¼‰:')
    print(f'  - å¹³å‡å¹´é¾„: {cluster_data["å¹´é¾„"].mean():.1f}å²')
    print(f'  - å¹³å‡å¹´æ”¶å…¥: Â¥{cluster_data["å¹´æ”¶å…¥"].mean():.0f}')
    print(f'  - å¹³å‡æ¶ˆè´¹è¯„åˆ†: {cluster_data["æ¶ˆè´¹è¯„åˆ†"].mean():.1f}')
    print(f'  - å®¢æˆ·æ•°é‡: {len(cluster_data)}')
```

**é¡¹ç›® 3ï¼šå›¾åƒå‹ç¼©**

```python
from sklearn.cluster import KMeans
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

def compress_image(image_path, n_colors):
    """ä½¿ç”¨k-Meanså‹ç¼©å›¾åƒ"""
    # åŠ è½½å›¾åƒ
    img = Image.open(image_path)
    img_array = np.array(img)

    # è·å–åŸå§‹å½¢çŠ¶
    h, w, c = img_array.shape

    # é‡å¡‘ä¸º(n_pixels, n_channels)
    pixels = img_array.reshape(-1, c)

    # k-Meansèšç±»
    kmeans = KMeans(n_clusters=n_colors, random_state=42, n_init=10)
    labels = kmeans.fit_predict(pixels)

    # ç”¨è´¨å¿ƒæ›¿æ¢åƒç´ å€¼
    compressed_pixels = kmeans.cluster_centers_[labels]

    # é‡å¡‘å›åŸå§‹å½¢çŠ¶
    compressed_img = compressed_pixels.reshape(h, w, c).astype(np.uint8)

    return img_array, compressed_img

# å‹ç¼©ç¤ºä¾‹
original, compressed = compress_image('image.jpg', n_colors=16)

# å¯è§†åŒ–
fig, axes = plt.subplots(1, 2, figsize=(12, 6))
axes[0].imshow(original)
axes[0].set_title('åŸå§‹å›¾åƒ')
axes[0].axis('off')

axes[1].imshow(compressed)
axes[1].set_title('å‹ç¼©å›¾åƒï¼ˆ16è‰²ï¼‰')
axes[1].axis('off')

plt.tight_layout()
plt.show()

# è®¡ç®—å‹ç¼©æ¯”
original_size = original.nbytes
compressed_size = 16 * 3 + len(original.flatten())  # è´¨å¿ƒ + æ ‡ç­¾
print(f'åŸå§‹å¤§å°: {original_size:,} å­—èŠ‚')
print(f'å‹ç¼©åå¤§å°: {compressed_size:,} å­—èŠ‚')
print(f'å‹ç¼©æ¯”: {original_size/compressed_size:.2f}x')
```

**é¡¹ç›® 4ï¼šè½®å»“åˆ†æ**

```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, silhouette_samples
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

def silhouette_analysis(X, k_range):
    """å¯¹ä¸åŒkå€¼è¿›è¡Œè½®å»“åˆ†æ"""
    for k in k_range:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

        # èšç±»
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)

        # è®¡ç®—è½®å»“ç³»æ•°
        silhouette_avg = silhouette_score(X, labels)
        sample_silhouette_values = silhouette_samples(X, labels)

        # ç»˜åˆ¶è½®å»“å›¾
        y_lower = 10
        for i in range(k):
            ith_cluster_silhouette_values = sample_silhouette_values[labels == i]
            ith_cluster_silhouette_values.sort()

            size_cluster_i = ith_cluster_silhouette_values.shape[0]
            y_upper = y_lower + size_cluster_i

            color = cm.nipy_spectral(float(i) / k)
            ax1.fill_betweenx(np.arange(y_lower, y_upper),
                            0, ith_cluster_silhouette_values,
                            facecolor=color, edgecolor=color, alpha=0.7)

            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
            y_lower = y_upper + 10

        ax1.set_title(f'è½®å»“å›¾ (k={k})')
        ax1.set_xlabel('è½®å»“ç³»æ•°')
        ax1.set_ylabel('ç°‡æ ‡ç­¾')
        ax1.axvline(x=silhouette_avg, color="red", linestyle="--",
                   label=f'å¹³å‡è½®å»“ç³»æ•°={silhouette_avg:.3f}')
        ax1.legend()

        # ç»˜åˆ¶ç°‡
        colors = cm.nipy_spectral(labels.astype(float) / k)
        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=colors)
        ax2.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
                   marker='X', c='red', s=200, alpha=1, edgecolors='black', linewidths=2)
        ax2.set_title(f'èšç±»ç»“æœ (k={k})')
        ax2.set_xlabel('ç‰¹å¾1')
        ax2.set_ylabel('ç‰¹å¾2')

        plt.tight_layout()
        plt.show()

# è¿è¡Œåˆ†æ
silhouette_analysis(X, range(2, 6))
```

## ğŸ¯ æµ‹ä¸€æµ‹

### é€‰æ‹©é¢˜

**1. k-Means ç®—æ³•çš„ç›®æ ‡æ˜¯ï¼š**

- A. æœ€å¤§åŒ–ç°‡é—´è·ç¦»
- B. æœ€å°åŒ–ç°‡å†…å¹³æ–¹å’Œ
- C. æœ€å¤§åŒ–è½®å»“ç³»æ•°
- D. æœ€å°åŒ–ç°‡æ•°

**2. k-Means çš„æ—¶é—´å¤æ‚åº¦æ˜¯ï¼š**

- A. O(n log n)
- B. O(nÂ²)
- C. O(n Ã— k Ã— i Ã— d)
- D. O(2â¿)

**3. è½®å»“ç³»æ•°çš„èŒƒå›´æ˜¯ï¼š**

- A. [0, 1]
- B. [-1, 1]
- C. [0, âˆ)
- D. (-âˆ, +âˆ)

**4. k-Means++æ”¹è¿›äº†ä»€ä¹ˆï¼Ÿ**

- A. æ”¶æ•›é€Ÿåº¦
- B. è´¨å¿ƒåˆå§‹åŒ–
- C. è·ç¦»è®¡ç®—
- D. ç°‡æ•°é€‰æ‹©

**5. ä»¥ä¸‹å“ªä¸ªä¸æ˜¯ k-Means çš„å‡è®¾ï¼Ÿ**

- A. ç°‡æ˜¯çƒå½¢çš„
- B. ç°‡å¤§å°ç›¸ä¼¼
- C. ç°‡å¯†åº¦ç›¸ä¼¼
- D. ç°‡å¯ä»¥é‡å 

**6. Mini-Batch k-Means çš„ä¼˜åŠ¿æ˜¯ï¼š**

- A. æ›´å‡†ç¡®
- B. æ›´å¿«
- C. æ›´ç¨³å®š
- D. ç°‡æ•°è‡ªåŠ¨ç¡®å®š

**7. k-Medoids ä¸ k-Means çš„ä¸»è¦åŒºåˆ«æ˜¯ï¼š**

- A. ä½¿ç”¨å®é™…æ•°æ®ç‚¹ä½œä¸ºä¸­å¿ƒ
- B. è®¡ç®—æ›´å¿«
- C. ä¸éœ€è¦è·ç¦»åº¦é‡
- D. å¯ä»¥è‡ªåŠ¨ç¡®å®š k

**8. è‚˜éƒ¨æ³•ç”¨äºï¼š**

- A. è®¡ç®—è·ç¦»
- B. é€‰æ‹© k å€¼
- C. åˆå§‹åŒ–è´¨å¿ƒ
- D. è¯„ä¼°æ€§èƒ½

### åˆ¤æ–­é¢˜

**1. k-Means æ€»æ˜¯æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜ã€‚** ï¼ˆÃ—ï¼‰

**2. k-Means é€‚åˆä»»æ„å½¢çŠ¶çš„ç°‡ã€‚** ï¼ˆÃ—ï¼‰

**3. k-Means++ä¿è¯æ›´å¥½çš„åˆå§‹åŒ–ã€‚** ï¼ˆâœ“ï¼‰

**4. è½®å»“ç³»æ•°è¶Šå¤§è¶Šå¥½ã€‚** ï¼ˆâœ“ï¼‰

**5. k-Means å¯ä»¥ç”¨äºå›å½’é—®é¢˜ã€‚** ï¼ˆÃ—ï¼‰

**6. k-Means å¯¹ç‰¹å¾ç¼©æ”¾æ•æ„Ÿã€‚** ï¼ˆâœ“ï¼‰

**7. è‚˜éƒ¨æ³•æ€»èƒ½æ˜ç¡®æŒ‡å‡ºæœ€ä¼˜ k å€¼ã€‚** ï¼ˆÃ—ï¼‰

**8. k-Means æ˜¯æ— ç›‘ç£å­¦ä¹ ç®—æ³•ã€‚** ï¼ˆâœ“ï¼‰

### ç®€ç­”é¢˜

**1. è§£é‡Š k-Means ç®—æ³•çš„å·¥ä½œåŸç†ã€‚**

**å‚è€ƒç­”æ¡ˆï¼š**
k-Means é€šè¿‡è¿­ä»£ä¼˜åŒ–å°†æ•°æ®åˆ†æˆ k ä¸ªç°‡ï¼š

1. éšæœºåˆå§‹åŒ– k ä¸ªè´¨å¿ƒ
2. åˆ†é…æ­¥éª¤ï¼šå°†æ¯ä¸ªæ•°æ®ç‚¹åˆ†é…ç»™æœ€è¿‘çš„è´¨å¿ƒ
3. æ›´æ–°æ­¥éª¤ï¼šé‡æ–°è®¡ç®—æ¯ä¸ªç°‡çš„è´¨å¿ƒï¼ˆç°‡å†…æ‰€æœ‰ç‚¹çš„å¹³å‡å€¼ï¼‰
4. é‡å¤æ­¥éª¤ 2-3 ç›´åˆ°è´¨å¿ƒä¸å†å˜åŒ–æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
   ç›®æ ‡æ˜¯æœ€å°åŒ–ç°‡å†…å¹³æ–¹å’Œï¼ˆWCSSï¼‰ã€‚

**2. k-Means æœ‰å“ªäº›å±€é™æ€§ï¼Ÿå¦‚ä½•å…‹æœï¼Ÿ**

**å‚è€ƒç­”æ¡ˆï¼š**
å±€é™æ€§ï¼š

1. éœ€è¦é¢„å…ˆæŒ‡å®š k - ä½¿ç”¨è‚˜éƒ¨æ³•ã€è½®å»“ç³»æ•°é€‰æ‹©
2. å¯¹åˆå§‹åŒ–æ•æ„Ÿ - ä½¿ç”¨ k-Means++ï¼Œå¤šæ¬¡è¿è¡Œ
3. å‡è®¾çƒå½¢ç°‡ - ä½¿ç”¨ DBSCANã€GMM ç­‰å…¶ä»–ç®—æ³•
4. å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ - é¢„å¤„ç†å»é™¤å¼‚å¸¸å€¼ï¼Œä½¿ç”¨ k-Medoids
5. å‡è®¾ç°‡å¤§å°ç›¸ä¼¼ - ä½¿ç”¨åŠ æƒ k-Means æˆ–å…¶ä»–æ–¹æ³•

**3. æ¯”è¾ƒè‚˜éƒ¨æ³•å’Œè½®å»“ç³»æ•°æ³•é€‰æ‹© k å€¼ã€‚**

**å‚è€ƒç­”æ¡ˆï¼š**
è‚˜éƒ¨æ³•ï¼š

- ç»˜åˆ¶ WCSS vs k æ›²çº¿ï¼Œå¯»æ‰¾"è‚˜éƒ¨"
- ä¼˜ç‚¹ï¼šç›´è§‚ã€è®¡ç®—ç®€å•
- ç¼ºç‚¹ï¼šè‚˜éƒ¨ä½ç½®å¯èƒ½ä¸æ˜æ˜¾

è½®å»“ç³»æ•°æ³•ï¼š

- è®¡ç®—æ¯ä¸ª k çš„å¹³å‡è½®å»“ç³»æ•°ï¼Œé€‰æ‹©æœ€å¤§å€¼
- ä¼˜ç‚¹ï¼šæœ‰æ˜ç¡®çš„æ•°å€¼æ ‡å‡†ï¼Œè€ƒè™‘ç°‡å†…ç´§å¯†åº¦å’Œç°‡é—´åˆ†ç¦»åº¦
- ç¼ºç‚¹ï¼šè®¡ç®—æˆæœ¬é«˜

å»ºè®®ï¼šç»“åˆå¤šç§æ–¹æ³•å¹¶è€ƒè™‘é¢†åŸŸçŸ¥è¯†ã€‚

### è®¡ç®—é¢˜

**1. ç»™å®šæ•°æ®ç‚¹ A(1,1), B(1,2), C(5,5), D(6,6)ï¼Œè´¨å¿ƒ Î¼â‚=(1,1.5), Î¼â‚‚=(5.5,5.5)ï¼Œè®¡ç®— WCSSã€‚**

**å‚è€ƒç­”æ¡ˆï¼š**
åˆ†é…ï¼š

- A åˆ° Î¼â‚ è·ç¦»=0.5ï¼Œåˆ° Î¼â‚‚ è·ç¦»=6.36 â†’ ç°‡ 1
- B åˆ° Î¼â‚ è·ç¦»=0.5ï¼Œåˆ° Î¼â‚‚ è·ç¦»=5.70 â†’ ç°‡ 1
- C åˆ° Î¼â‚ è·ç¦»=5.70ï¼Œåˆ° Î¼â‚‚ è·ç¦»=0.71 â†’ ç°‡ 2
- D åˆ° Î¼â‚ è·ç¦»=6.36ï¼Œåˆ° Î¼â‚‚ è·ç¦»=0.71 â†’ ç°‡ 2

WCSSï¼š

- ç°‡ 1ï¼š(1-1)Â²+(1-1.5)Â² + (1-1)Â²+(2-1.5)Â² = 0.25 + 0.25 = 0.5
- ç°‡ 2ï¼š(5-5.5)Â²+(5-5.5)Â² + (6-5.5)Â²+(6-5.5)Â² = 0.5 + 0.5 = 1.0
- æ€» WCSS = 1.5

**2. è®¡ç®—è½®å»“ç³»æ•°ï¼šç‚¹ x åˆ°åŒç°‡å…¶ä»–ç‚¹å¹³å‡è·ç¦» a=1.5ï¼Œåˆ°æœ€è¿‘å…¶ä»–ç°‡å¹³å‡è·ç¦» b=3ã€‚**

**å‚è€ƒç­”æ¡ˆï¼š**
$$s = \frac{b - a}{\max(a, b)} = \frac{3 - 1.5}{\max(1.5, 3)} = \frac{1.5}{3} = 0.5$$

è½®å»“ç³»æ•°ä¸º 0.5ï¼Œè¡¨ç¤ºèšç±»è¾ƒå¥½ã€‚

## ğŸ—ºï¸ æ€ç»´å¯¼å›¾

```
èšç±»ä¸k-Means
â”‚
â”œâ”€â”€â”€ èšç±»æ¦‚å¿µ
â”‚    â”œâ”€â”€â”€ æ— ç›‘ç£å­¦ä¹ 
â”‚    â”œâ”€â”€â”€ ç°‡ï¼ˆç›¸ä¼¼å¯¹è±¡ç»„ï¼‰
â”‚    â”œâ”€â”€â”€ ç›®æ ‡ï¼šç»„å†…ç›¸ä¼¼ï¼Œç»„é—´ä¸åŒ
â”‚    â””â”€â”€â”€ åº”ç”¨ï¼šç»†åˆ†ã€å‹ç¼©ã€å¼‚å¸¸æ£€æµ‹
â”‚
â”œâ”€â”€â”€ èšç±»ç±»å‹
â”‚    â”œâ”€â”€â”€ åˆ’åˆ†èšç±»ï¼ˆk-Meansï¼‰
â”‚    â”œâ”€â”€â”€ å±‚æ¬¡èšç±»ï¼ˆæ ‘çŠ¶å›¾ï¼‰
â”‚    â”œâ”€â”€â”€ å¯†åº¦èšç±»ï¼ˆDBSCANï¼‰
â”‚    â”œâ”€â”€â”€ ç½‘æ ¼èšç±»
â”‚    â””â”€â”€â”€ æ¨¡å‹èšç±»ï¼ˆGMMï¼‰
â”‚
â”œâ”€â”€â”€ k-Meansç®—æ³•
â”‚    â”œâ”€â”€â”€ åŸºæœ¬æ­¥éª¤
â”‚    â”‚    â”œâ”€â”€â”€ 1. åˆå§‹åŒ–è´¨å¿ƒ
â”‚    â”‚    â”œâ”€â”€â”€ 2. åˆ†é…ç‚¹åˆ°æœ€è¿‘è´¨å¿ƒ
â”‚    â”‚    â”œâ”€â”€â”€ 3. æ›´æ–°è´¨å¿ƒ
â”‚    â”‚    â””â”€â”€â”€ 4. é‡å¤ç›´åˆ°æ”¶æ•›
â”‚    â”œâ”€â”€â”€ ç›®æ ‡å‡½æ•°
â”‚    â”‚    â””â”€â”€â”€ æœ€å°åŒ–WCSS
â”‚    â”œâ”€â”€â”€ æ—¶é—´å¤æ‚åº¦
â”‚    â”‚    â””â”€â”€â”€ O(nÃ—kÃ—iÃ—d)
â”‚    â””â”€â”€â”€ æ”¶æ•›ä¿è¯
â”‚         â””â”€â”€â”€ å±€éƒ¨æœ€ä¼˜
â”‚
â”œâ”€â”€â”€ k-Meanså˜ä½“
â”‚    â”œâ”€â”€â”€ k-Means++
â”‚    â”‚    â”œâ”€â”€â”€ æ”¹è¿›åˆå§‹åŒ–
â”‚    â”‚    â””â”€â”€â”€ æ›´å¿«æ”¶æ•›
â”‚    â”œâ”€â”€â”€ Mini-Batch k-Means
â”‚    â”‚    â”œâ”€â”€â”€ ä½¿ç”¨å°æ‰¹é‡
â”‚    â”‚    â””â”€â”€â”€ æ›´å¿«é€Ÿåº¦
â”‚    â”œâ”€â”€â”€ k-Medoids
â”‚    â”‚    â”œâ”€â”€â”€ ä½¿ç”¨å®é™…ç‚¹
â”‚    â”‚    â””â”€â”€â”€ æ›´ç¨³å¥
â”‚    â””â”€â”€â”€ Fuzzy k-Means
â”‚         â””â”€â”€â”€ è½¯èšç±»
â”‚
â”œâ”€â”€â”€ é€‰æ‹©kå€¼
â”‚    â”œâ”€â”€â”€ è‚˜éƒ¨æ³•
â”‚    â”‚    â””â”€â”€â”€ WCSS vs kæ›²çº¿
â”‚    â”œâ”€â”€â”€ è½®å»“ç³»æ•°
â”‚    â”‚    â””â”€â”€â”€ èŒƒå›´[-1, 1]
â”‚    â”œâ”€â”€â”€ é—´éš™ç»Ÿè®¡
â”‚    â”œâ”€â”€â”€ Davies-BouldinæŒ‡æ•°
â”‚    â”‚    â””â”€â”€â”€ è¶Šå°è¶Šå¥½
â”‚    â””â”€â”€â”€ Calinski-HarabaszæŒ‡æ•°
â”‚         â””â”€â”€â”€ è¶Šå¤§è¶Šå¥½
â”‚
â”œâ”€â”€â”€ ä¼˜ç‚¹
â”‚    â”œâ”€â”€â”€ ç®€å•ç›´è§‚
â”‚    â”œâ”€â”€â”€ é«˜æ•ˆå¯æ‰©å±•
â”‚    â”œâ”€â”€â”€ ä¿è¯æ”¶æ•›
â”‚    â””â”€â”€â”€ æ˜“äºå®ç°
â”‚
â”œâ”€â”€â”€ ç¼ºç‚¹
â”‚    â”œâ”€â”€â”€ éœ€è¦æŒ‡å®šk
â”‚    â”œâ”€â”€â”€ å¯¹åˆå§‹åŒ–æ•æ„Ÿ
â”‚    â”œâ”€â”€â”€ å‡è®¾çƒå½¢ç°‡
â”‚    â”œâ”€â”€â”€ å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ
â”‚    â””â”€â”€â”€ å‡è®¾ç°‡å¤§å°ç›¸ä¼¼
â”‚
â”œâ”€â”€â”€ åº”ç”¨åœºæ™¯
â”‚    â”œâ”€â”€â”€ å®¢æˆ·ç»†åˆ†
â”‚    â”‚    â””â”€â”€â”€ è¥é”€ç­–ç•¥
â”‚    â”œâ”€â”€â”€ å›¾åƒå‹ç¼©
â”‚    â”‚    â””â”€â”€â”€ å‡å°‘é¢œè‰²
â”‚    â”œâ”€â”€â”€ æ–‡æ¡£èšç±»
â”‚    â”‚    â””â”€â”€â”€ ä¸»é¢˜å‘ç°
â”‚    â”œâ”€â”€â”€ æ¨èç³»ç»Ÿ
â”‚    â”‚    â””â”€â”€â”€ ååŒè¿‡æ»¤
â”‚    â””â”€â”€â”€ å¼‚å¸¸æ£€æµ‹
â”‚         â””â”€â”€â”€ è¯†åˆ«ç¦»ç¾¤ç°‡
â”‚
â””â”€â”€â”€ å®ç”¨æŠ€å·§
     â”œâ”€â”€â”€ ç‰¹å¾æ ‡å‡†åŒ–
     â”œâ”€â”€â”€ å»é™¤å¼‚å¸¸å€¼
     â”œâ”€â”€â”€ å¤šæ¬¡è¿è¡Œ
     â”œâ”€â”€â”€ ä½¿ç”¨k-Means++
     â””â”€â”€â”€ ç»“åˆé¢†åŸŸçŸ¥è¯†
```

## ğŸ“– å­¦ä¹ èµ„æº

### è¯¾ç¨‹ææ–™

- **06_CST8502_Clustering_kMeans1.pdf**ï¼šèšç±»å’Œ k-Means ç»¼åˆæŒ‡å—

### Python åº“

- **scikit-learn**ï¼šKMeans, MiniBatchKMeans
- **scipy**ï¼šå±‚æ¬¡èšç±»
- **sklearn.metrics**ï¼šè¯„ä¼°æŒ‡æ ‡
- **matplotlib/seaborn**ï¼šå¯è§†åŒ–

### ç»å…¸è®ºæ–‡

- MacQueen, J. (1967). "Some Methods for Classification and Analysis of Multivariate Observations"
- Arthur, D. & Vassilvitskii, S. (2007). "k-means++: The Advantages of Careful Seeding"

### æ¨èé˜…è¯»

- ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ç¬¬ 14 ç«  - æèˆª
- "Introduction to Statistical Learning" - Chapter 10

## ğŸ¯ å­¦ä¹ ç›®æ ‡æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬ç« åï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿï¼š

- âœ… ç†è§£èšç±»çš„æ¦‚å¿µå’Œä¸åŒç±»å‹
- âœ… æŒæ¡ k-Means ç®—æ³•çš„å·¥ä½œåŸç†
- âœ… å®ç° k-Means ç®—æ³•ï¼ˆä»é›¶å¼€å§‹ï¼‰
- âœ… ä½¿ç”¨å¤šç§æ–¹æ³•é€‰æ‹©æœ€ä¼˜ k å€¼
- âœ… åº”ç”¨ k-Means++æ”¹è¿›åˆå§‹åŒ–
- âœ… ç†è§£ k-Means çš„ä¼˜åŠ¿å’Œå±€é™æ€§
- âœ… è¯„ä¼°èšç±»è´¨é‡ï¼ˆWCSSã€è½®å»“ç³»æ•°ï¼‰
- âœ… å°† k-Means åº”ç”¨äºå®é™…é—®é¢˜ï¼ˆå®¢æˆ·ç»†åˆ†ã€å›¾åƒå‹ç¼©ï¼‰
- âœ… å¯è§†åŒ–èšç±»ç»“æœ
- âœ… æ¯”è¾ƒä¸åŒèšç±»ç®—æ³•

---

**è¯¾ç¨‹æ€»ç»“ï¼š** æ­å–œå®Œæˆ CST8502 æœºå™¨å­¦ä¹ è¯¾ç¨‹çš„ä¸»è¦ç« èŠ‚ï¼æ‚¨å·²ç»å­¦ä¹ äº†ä»æ•°æ®é¢„å¤„ç†åˆ°åˆ†ç±»ã€å¼‚å¸¸æ£€æµ‹å’Œèšç±»çš„å®Œæ•´æœºå™¨å­¦ä¹ æµç¨‹ã€‚ç»§ç»­å®è·µå’Œæ¢ç´¢æ›´é«˜çº§çš„ä¸»é¢˜ï¼Œå¦‚é›†æˆæ–¹æ³•ã€æ·±åº¦å­¦ä¹ å’Œæ¨¡å‹éƒ¨ç½²ã€‚
